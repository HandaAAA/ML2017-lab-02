{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_SGD: 19.9698601044\n",
      "Loss_SGD: 15.7931829676\n",
      "Loss_SGD: 12.4271389964\n",
      "Loss_SGD: 9.72212109805\n",
      "Loss_SGD: 7.60698377551\n",
      "Loss_SGD: 5.98275175425\n",
      "Loss_SGD: 4.75814948171\n",
      "Loss_SGD: 3.84988410511\n",
      "Loss_SGD: 3.16704820618\n",
      "Loss_SGD: 2.63841434426\n",
      "Loss_SGD: 2.22554983802\n",
      "Loss_SGD: 1.90045152616\n",
      "Loss_SGD: 1.64543051321\n",
      "Loss_SGD: 1.43923565536\n",
      "Loss_SGD: 1.27368736053\n",
      "Loss_SGD: 1.14112021034\n",
      "Loss_SGD: 1.03417137882\n",
      "Loss_SGD: 0.948438584051\n",
      "Loss_SGD: 0.878741123542\n",
      "Loss_SGD: 0.822686603866\n",
      "Loss_SGD: 0.777257769932\n",
      "Loss_SGD: 0.740651842552\n",
      "Loss_SGD: 0.711087701898\n",
      "Loss_SGD: 0.686933064373\n",
      "Loss_SGD: 0.667396069207\n",
      "Loss_SGD: 0.651791179798\n",
      "Loss_SGD: 0.639232602134\n",
      "Loss_SGD: 0.629184339294\n",
      "Loss_SGD: 0.620488114694\n",
      "Loss_SGD: 0.613709629503\n",
      "Loss_SGD: 0.608342718461\n",
      "Loss_SGD: 0.603956067911\n",
      "Loss_SGD: 0.600381402797\n",
      "Loss_SGD: 0.597477174005\n",
      "Loss_SGD: 0.595144890837\n",
      "Loss_SGD: 0.593187926451\n",
      "Loss_SGD: 0.591679959415\n",
      "Loss_SGD: 0.590411294754\n",
      "Loss_SGD: 0.589426306145\n",
      "Loss_SGD: 0.588608788204\n",
      "Loss_SGD: 0.587945195313\n",
      "Loss_SGD: 0.587433454324\n",
      "Loss_SGD: 0.587031525363\n",
      "Loss_SGD: 0.586679669363\n",
      "Loss_SGD: 0.586396468844\n",
      "Loss_SGD: 0.586147463852\n",
      "Loss_SGD: 0.585987177824\n",
      "Loss_SGD: 0.585845848371\n",
      "Loss_SGD: 0.585707502525\n",
      "Loss_SGD: 0.585698237761\n",
      "Loss_SGD: 0.58558479037\n",
      "Loss_SGD: 0.585548766454\n",
      "Loss_SGD: 0.585664695985\n",
      "Loss_SGD: 0.585579561939\n",
      "Loss_SGD: 0.585377598197\n",
      "Loss_SGD: 0.585323284533\n",
      "Loss_SGD: 0.58535408245\n",
      "Loss_SGD: 0.585334317081\n",
      "Loss_SGD: 0.585236621952\n",
      "Loss_SGD: 0.585329329072\n",
      "Loss_SGD: 0.585261387682\n",
      "Loss_SGD: 0.585220689964\n",
      "Loss_SGD: 0.585272884375\n",
      "Loss_SGD: 0.585335575966\n",
      "Loss_SGD: 0.585365246587\n",
      "Loss_SGD: 0.58518315343\n",
      "Loss_SGD: 0.585219614343\n",
      "Loss_SGD: 0.585292613635\n",
      "Loss_SGD: 0.585299111798\n",
      "Loss_SGD: 0.58539657154\n",
      "Loss_SGD: 0.585269343413\n",
      "Loss_SGD: 0.58538564599\n",
      "Loss_SGD: 0.585261730207\n",
      "Loss_SGD: 0.585262535103\n",
      "Loss_SGD: 0.585197177524\n",
      "Loss_SGD: 0.585215294288\n",
      "Loss_SGD: 0.585197705587\n",
      "Loss_SGD: 0.585190598538\n",
      "Loss_SGD: 0.585215555892\n",
      "Loss_SGD: 0.585194682385\n",
      "Loss_SGD: 0.585222649011\n",
      "Loss_SGD: 0.585293457366\n",
      "Loss_SGD: 0.585407991079\n",
      "Loss_SGD: 0.585469089502\n",
      "Loss_SGD: 0.585499764805\n",
      "Loss_SGD: 0.58549832241\n",
      "Loss_SGD: 0.585348897782\n",
      "Loss_SGD: 0.585188425351\n",
      "Loss_SGD: 0.585179546018\n",
      "Loss_SGD: 0.585199941652\n",
      "Loss_SGD: 0.585200008865\n",
      "Loss_SGD: 0.585165318154\n",
      "Loss_SGD: 0.585189358316\n",
      "Loss_SGD: 0.585187111575\n",
      "Loss_SGD: 0.585229041745\n",
      "Loss_SGD: 0.585183917251\n",
      "Loss_SGD: 0.58518763326\n",
      "Loss_SGD: 0.585206655333\n",
      "Loss_SGD: 0.58519766852\n",
      "Loss_SGD: 0.585252490151\n",
      "Loss_SGD: 0.58532413661\n",
      "Loss_SGD: 0.585282162484\n",
      "Loss_SGD: 0.585197496795\n",
      "Loss_SGD: 0.58532711905\n",
      "Loss_SGD: 0.585395589391\n",
      "Loss_SGD: 0.585481206568\n",
      "Loss_SGD: 0.585420510823\n",
      "Loss_SGD: 0.585330248043\n",
      "Loss_SGD: 0.585339315331\n",
      "Loss_SGD: 0.585348916087\n",
      "Loss_SGD: 0.585438849772\n",
      "Loss_SGD: 0.585477389146\n",
      "Loss_SGD: 0.585692820563\n",
      "Loss_SGD: 0.585330986816\n",
      "Loss_SGD: 0.585275870351\n",
      "Loss_SGD: 0.585451600645\n",
      "Loss_SGD: 0.585258812028\n",
      "Loss_SGD: 0.585560891186\n",
      "Loss_SGD: 0.58537127146\n",
      "Loss_SGD: 0.585287171098\n",
      "Loss_SGD: 0.585325196248\n",
      "Loss_SGD: 0.585516659482\n",
      "Loss_SGD: 0.585497427013\n",
      "Loss_SGD: 0.585368091788\n",
      "Loss_SGD: 0.585345886804\n",
      "Loss_SGD: 0.58529699127\n",
      "Loss_SGD: 0.585338503763\n",
      "Loss_SGD: 0.585271168176\n",
      "Loss_SGD: 0.585202699309\n",
      "Loss_SGD: 0.585203443395\n",
      "Loss_SGD: 0.58521238071\n",
      "Loss_SGD: 0.585205217369\n",
      "Loss_SGD: 0.585277859384\n",
      "Loss_SGD: 0.585265722095\n",
      "Loss_SGD: 0.585290202384\n",
      "Loss_SGD: 0.585313902598\n",
      "Loss_SGD: 0.585219336479\n",
      "Loss_SGD: 0.585227239454\n",
      "Loss_SGD: 0.585227871508\n",
      "Loss_SGD: 0.585212903952\n",
      "Loss_SGD: 0.585219839248\n",
      "Loss_SGD: 0.585217353607\n",
      "Loss_SGD: 0.585198250666\n",
      "Loss_SGD: 0.585181754741\n",
      "Loss_SGD: 0.58519203832\n",
      "Loss_SGD: 0.585189908976\n",
      "Loss_SGD: 0.585215714093\n",
      "Loss_SGD: 0.585342037566\n",
      "Loss_SGD: 0.58529134554\n",
      "Loss_SGD: 0.58529130931\n",
      "Loss_SGD: 0.585242168547\n",
      "Loss_SGD: 0.585266255092\n",
      "Loss_SGD: 0.585280370189\n",
      "Loss_SGD: 0.585385576138\n",
      "Loss_SGD: 0.585304810158\n",
      "Loss_SGD: 0.585355036445\n",
      "Loss_SGD: 0.58542283564\n",
      "Loss_SGD: 0.585274320874\n",
      "Loss_SGD: 0.585233143428\n",
      "Loss_SGD: 0.585215283114\n",
      "Loss_SGD: 0.585248772589\n",
      "Loss_SGD: 0.585303962609\n",
      "Loss_SGD: 0.585198208592\n",
      "Loss_SGD: 0.585303295286\n",
      "Loss_SGD: 0.585203074894\n",
      "Loss_SGD: 0.58518997714\n",
      "Loss_SGD: 0.585183533054\n",
      "Loss_SGD: 0.585204063573\n",
      "Loss_SGD: 0.585186906866\n",
      "Loss_SGD: 0.585204163669\n",
      "Loss_SGD: 0.585179955485\n",
      "Loss_SGD: 0.585177556459\n",
      "Loss_SGD: 0.585152816115\n",
      "Loss_SGD: 0.58517062281\n",
      "Loss_SGD: 0.585174046603\n",
      "Loss_SGD: 0.585334348081\n",
      "Loss_SGD: 0.585365626768\n",
      "Loss_SGD: 0.585374855211\n",
      "Loss_SGD: 0.585261794134\n",
      "Loss_SGD: 0.585208067857\n",
      "Loss_SGD: 0.585206220827\n",
      "Loss_SGD: 0.58516414846\n",
      "Loss_SGD: 0.585190533983\n",
      "Loss_SGD: 0.585172883838\n",
      "Loss_SGD: 0.585192272377\n",
      "Loss_SGD: 0.585188252077\n",
      "Loss_SGD: 0.585262714182\n",
      "Loss_SGD: 0.585214846196\n",
      "Loss_SGD: 0.585188066898\n",
      "Loss_SGD: 0.585195339812\n",
      "Loss_SGD: 0.585206489925\n",
      "Loss_SGD: 0.585202136026\n",
      "Loss_SGD: 0.585218920056\n",
      "Loss_SGD: 0.585221777378\n",
      "Loss_SGD: 0.585207429761\n",
      "Loss_SGD: 0.585198319369\n",
      "Loss_SGD: 0.585196915877\n",
      "Loss_SGD: 0.585169403624\n",
      "Loss_SGD: 0.585201681769\n",
      "Loss_SGD: 0.585207708019\n",
      "Loss_NAG: 19.9983137395\n",
      "Loss_NAG: 12.4608843476\n",
      "Loss_NAG: 5.62926892167\n",
      "Loss_NAG: 2.01846615505\n",
      "Loss_NAG: 1.20713829851\n",
      "Loss_NAG: 1.70971199126\n",
      "Loss_NAG: 2.53758503406\n",
      "Loss_NAG: 3.06401949767\n",
      "Loss_NAG: 3.08466816113\n",
      "Loss_NAG: 2.70778182082\n",
      "Loss_NAG: 2.18299757465\n",
      "Loss_NAG: 1.64611091913\n",
      "Loss_NAG: 1.1600392761\n",
      "Loss_NAG: 0.799025863041\n",
      "Loss_NAG: 0.623852153309\n",
      "Loss_NAG: 0.620079617401\n",
      "Loss_NAG: 0.710182227573\n",
      "Loss_NAG: 0.806816078217\n",
      "Loss_NAG: 0.851873500329\n",
      "Loss_NAG: 0.834776217257\n",
      "Loss_NAG: 0.774841494698\n",
      "Loss_NAG: 0.700938958316\n",
      "Loss_NAG: 0.638646105694\n",
      "Loss_NAG: 0.600456232449\n",
      "Loss_NAG: 0.586702879507\n",
      "Loss_NAG: 0.59078521595\n",
      "Loss_NAG: 0.602700507762\n",
      "Loss_NAG: 0.613402124725\n",
      "Loss_NAG: 0.617711969578\n",
      "Loss_NAG: 0.615003539663\n",
      "Loss_NAG: 0.607448440609\n",
      "Loss_NAG: 0.598529651017\n",
      "Loss_NAG: 0.59122783939\n",
      "Loss_NAG: 0.587075837228\n",
      "Loss_NAG: 0.585886604669\n",
      "Loss_NAG: 0.586459528098\n",
      "Loss_NAG: 0.587771814565\n",
      "Loss_NAG: 0.588906154105\n",
      "Loss_NAG: 0.58933654907\n",
      "Loss_NAG: 0.588948075794\n",
      "Loss_NAG: 0.588034558766\n",
      "Loss_NAG: 0.586864457867\n",
      "Loss_NAG: 0.585966872547\n",
      "Loss_NAG: 0.585530553958\n",
      "Loss_NAG: 0.585466622255\n",
      "Loss_NAG: 0.585630480635\n",
      "Loss_NAG: 0.585806798411\n",
      "Loss_NAG: 0.585822629454\n",
      "Loss_NAG: 0.585739555582\n",
      "Loss_NAG: 0.585686728341\n",
      "Loss_NAG: 0.585604148431\n",
      "Loss_NAG: 0.585514097623\n",
      "Loss_NAG: 0.585489517976\n",
      "Loss_NAG: 0.585420828815\n",
      "Loss_NAG: 0.585311161143\n",
      "Loss_NAG: 0.585338541531\n",
      "Loss_NAG: 0.585442891403\n",
      "Loss_NAG: 0.585465898137\n",
      "Loss_NAG: 0.585402487088\n",
      "Loss_NAG: 0.585453563714\n",
      "Loss_NAG: 0.585564455738\n",
      "Loss_NAG: 0.585657123478\n",
      "Loss_NAG: 0.585540751904\n",
      "Loss_NAG: 0.585337598333\n",
      "Loss_NAG: 0.585287925669\n",
      "Loss_NAG: 0.585321936378\n",
      "Loss_NAG: 0.585331780997\n",
      "Loss_NAG: 0.585286655679\n",
      "Loss_NAG: 0.585302833571\n",
      "Loss_NAG: 0.58547112614\n",
      "Loss_NAG: 0.585551142531\n",
      "Loss_NAG: 0.585516017567\n",
      "Loss_NAG: 0.585686650447\n",
      "Loss_NAG: 0.585723755225\n",
      "Loss_NAG: 0.585519254837\n",
      "Loss_NAG: 0.58535872181\n",
      "Loss_NAG: 0.585313983044\n",
      "Loss_NAG: 0.585259526697\n",
      "Loss_NAG: 0.585259115194\n",
      "Loss_NAG: 0.585328746866\n",
      "Loss_NAG: 0.585438368061\n",
      "Loss_NAG: 0.585590598432\n",
      "Loss_NAG: 0.585500854891\n",
      "Loss_NAG: 0.585479284739\n",
      "Loss_NAG: 0.585457439261\n",
      "Loss_NAG: 0.585582995398\n",
      "Loss_NAG: 0.586148391771\n",
      "Loss_NAG: 0.586353143932\n",
      "Loss_NAG: 0.585899319989\n",
      "Loss_NAG: 0.585405870335\n",
      "Loss_NAG: 0.585291781122\n",
      "Loss_NAG: 0.585419504337\n",
      "Loss_NAG: 0.585365301016\n",
      "Loss_NAG: 0.585356674541\n",
      "Loss_NAG: 0.585453016161\n",
      "Loss_NAG: 0.585738658609\n",
      "Loss_NAG: 0.586048503873\n",
      "Loss_NAG: 0.585711615426\n",
      "Loss_NAG: 0.585516446243\n",
      "Loss_NAG: 0.585468940361\n",
      "Loss_NAG: 0.58533296821\n",
      "Loss_NAG: 0.585322513278\n",
      "Loss_NAG: 0.585368931104\n",
      "Loss_NAG: 0.585336879309\n",
      "Loss_NAG: 0.585320599586\n",
      "Loss_NAG: 0.585302614643\n",
      "Loss_NAG: 0.585392172754\n",
      "Loss_NAG: 0.585401659324\n",
      "Loss_NAG: 0.585357422573\n",
      "Loss_NAG: 0.585404394295\n",
      "Loss_NAG: 0.58534672833\n",
      "Loss_NAG: 0.585350669821\n",
      "Loss_NAG: 0.585342626037\n",
      "Loss_NAG: 0.585362892162\n",
      "Loss_NAG: 0.585464426991\n",
      "Loss_NAG: 0.585495964028\n",
      "Loss_NAG: 0.585464039434\n",
      "Loss_NAG: 0.585368236078\n",
      "Loss_NAG: 0.585339577028\n",
      "Loss_NAG: 0.585294653389\n",
      "Loss_NAG: 0.585295136141\n",
      "Loss_NAG: 0.585343911754\n",
      "Loss_NAG: 0.585383235728\n",
      "Loss_NAG: 0.585559668559\n",
      "Loss_NAG: 0.585613759802\n",
      "Loss_NAG: 0.585757689025\n",
      "Loss_NAG: 0.585456387405\n",
      "Loss_NAG: 0.585244576758\n",
      "Loss_NAG: 0.585335515437\n",
      "Loss_NAG: 0.585427285593\n",
      "Loss_NAG: 0.585391565765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_NAG: 0.585305143054\n",
      "Loss_NAG: 0.585276795051\n",
      "Loss_NAG: 0.585270281482\n",
      "Loss_NAG: 0.58528376461\n",
      "Loss_NAG: 0.585315574722\n",
      "Loss_NAG: 0.585320460827\n",
      "Loss_NAG: 0.585571782072\n",
      "Loss_NAG: 0.58559921061\n",
      "Loss_NAG: 0.585284292914\n",
      "Loss_NAG: 0.585253275477\n",
      "Loss_NAG: 0.58529887776\n",
      "Loss_NAG: 0.585312109025\n",
      "Loss_NAG: 0.585332509563\n",
      "Loss_NAG: 0.585455549695\n",
      "Loss_NAG: 0.585700120359\n",
      "Loss_NAG: 0.585858308059\n",
      "Loss_NAG: 0.585631649523\n",
      "Loss_NAG: 0.585417936336\n",
      "Loss_NAG: 0.585372115062\n",
      "Loss_NAG: 0.585332114299\n",
      "Loss_NAG: 0.585283365482\n",
      "Loss_NAG: 0.585264619101\n",
      "Loss_NAG: 0.585266667547\n",
      "Loss_NAG: 0.585309810905\n",
      "Loss_NAG: 0.585304599265\n",
      "Loss_NAG: 0.585290348422\n",
      "Loss_NAG: 0.585363803461\n",
      "Loss_NAG: 0.585668393218\n",
      "Loss_NAG: 0.585698255348\n",
      "Loss_NAG: 0.585607739347\n",
      "Loss_NAG: 0.585427686323\n",
      "Loss_NAG: 0.585359663885\n",
      "Loss_NAG: 0.585342520625\n",
      "Loss_NAG: 0.585350851193\n",
      "Loss_NAG: 0.585311080417\n",
      "Loss_NAG: 0.585268761617\n",
      "Loss_NAG: 0.585264305141\n",
      "Loss_NAG: 0.585369075684\n",
      "Loss_NAG: 0.585600006472\n",
      "Loss_NAG: 0.585783337374\n",
      "Loss_NAG: 0.585628836985\n",
      "Loss_NAG: 0.585406174478\n",
      "Loss_NAG: 0.585396700732\n",
      "Loss_NAG: 0.585433335492\n",
      "Loss_NAG: 0.585523574436\n",
      "Loss_NAG: 0.585678473991\n",
      "Loss_NAG: 0.58574668392\n",
      "Loss_NAG: 0.585567812799\n",
      "Loss_NAG: 0.585475027552\n",
      "Loss_NAG: 0.585428165654\n",
      "Loss_NAG: 0.585417929453\n",
      "Loss_NAG: 0.585395866518\n",
      "Loss_NAG: 0.585383478789\n",
      "Loss_NAG: 0.585338922649\n",
      "Loss_NAG: 0.585315769844\n",
      "Loss_NAG: 0.585303875744\n",
      "Loss_NAG: 0.585381269219\n",
      "Loss_NAG: 0.58528264292\n",
      "Loss_NAG: 0.585305464978\n",
      "Loss_NAG: 0.58545359224\n",
      "Loss_NAG: 0.585372146072\n",
      "Loss_NAG: 0.585292480344\n",
      "Loss_NAG: 0.585320556584\n",
      "Loss_NAG: 0.585363231159\n",
      "Loss_NAG: 0.58537081048\n",
      "Loss_NAG: 0.585400607048\n",
      "Loss_NAG: 0.585487086412\n",
      "Loss_NAG: 0.585750531446\n",
      "Loss_NAG: 0.585888955501\n",
      "Loss_RMSProp: 22.9375493365\n",
      "Loss_RMSProp: 21.4520301667\n",
      "Loss_RMSProp: 20.2718565425\n",
      "Loss_RMSProp: 19.2652787101\n",
      "Loss_RMSProp: 18.378404743\n",
      "Loss_RMSProp: 17.5812123672\n",
      "Loss_RMSProp: 16.8465497161\n",
      "Loss_RMSProp: 16.1610709651\n",
      "Loss_RMSProp: 15.5249355722\n",
      "Loss_RMSProp: 14.919791756\n",
      "Loss_RMSProp: 14.3504928943\n",
      "Loss_RMSProp: 13.809088325\n",
      "Loss_RMSProp: 13.2920870668\n",
      "Loss_RMSProp: 12.7967983618\n",
      "Loss_RMSProp: 12.3220122937\n",
      "Loss_RMSProp: 11.8662360609\n",
      "Loss_RMSProp: 11.4273819676\n",
      "Loss_RMSProp: 11.0030875706\n",
      "Loss_RMSProp: 10.5978396511\n",
      "Loss_RMSProp: 10.2059569046\n",
      "Loss_RMSProp: 9.82176054802\n",
      "Loss_RMSProp: 9.45721187553\n",
      "Loss_RMSProp: 9.09956320183\n",
      "Loss_RMSProp: 8.7529985854\n",
      "Loss_RMSProp: 8.42130865149\n",
      "Loss_RMSProp: 8.09784398758\n",
      "Loss_RMSProp: 7.7881594601\n",
      "Loss_RMSProp: 7.48624957028\n",
      "Loss_RMSProp: 7.19495512354\n",
      "Loss_RMSProp: 6.91259146434\n",
      "Loss_RMSProp: 6.63944984613\n",
      "Loss_RMSProp: 6.37688844035\n",
      "Loss_RMSProp: 6.11901753619\n",
      "Loss_RMSProp: 5.87331341007\n",
      "Loss_RMSProp: 5.63613165955\n",
      "Loss_RMSProp: 5.40811622216\n",
      "Loss_RMSProp: 5.18895179747\n",
      "Loss_RMSProp: 4.97899108064\n",
      "Loss_RMSProp: 4.77543508719\n",
      "Loss_RMSProp: 4.57911222792\n",
      "Loss_RMSProp: 4.3899904831\n",
      "Loss_RMSProp: 4.20785462858\n",
      "Loss_RMSProp: 4.03400002515\n",
      "Loss_RMSProp: 3.8665767059\n",
      "Loss_RMSProp: 3.70696052733\n",
      "Loss_RMSProp: 3.5530290592\n",
      "Loss_RMSProp: 3.40601402965\n",
      "Loss_RMSProp: 3.26421151191\n",
      "Loss_RMSProp: 3.12724013565\n",
      "Loss_RMSProp: 2.99652199007\n",
      "Loss_RMSProp: 2.871302402\n",
      "Loss_RMSProp: 2.75041960913\n",
      "Loss_RMSProp: 2.63518437791\n",
      "Loss_RMSProp: 2.52490488682\n",
      "Loss_RMSProp: 2.41877443599\n",
      "Loss_RMSProp: 2.31769747587\n",
      "Loss_RMSProp: 2.21992378615\n",
      "Loss_RMSProp: 2.12691476651\n",
      "Loss_RMSProp: 2.03737328762\n",
      "Loss_RMSProp: 1.95113735554\n",
      "Loss_RMSProp: 1.87019566076\n",
      "Loss_RMSProp: 1.79145457368\n",
      "Loss_RMSProp: 1.71646561268\n",
      "Loss_RMSProp: 1.64539675406\n",
      "Loss_RMSProp: 1.57781529056\n",
      "Loss_RMSProp: 1.51426482924\n",
      "Loss_RMSProp: 1.45364252511\n",
      "Loss_RMSProp: 1.39602813993\n",
      "Loss_RMSProp: 1.34107368738\n",
      "Loss_RMSProp: 1.28888615569\n",
      "Loss_RMSProp: 1.24017109965\n",
      "Loss_RMSProp: 1.19323634139\n",
      "Loss_RMSProp: 1.14930333911\n",
      "Loss_RMSProp: 1.10758818109\n",
      "Loss_RMSProp: 1.06866960416\n",
      "Loss_RMSProp: 1.03151477678\n",
      "Loss_RMSProp: 0.996925738886\n",
      "Loss_RMSProp: 0.964356741543\n",
      "Loss_RMSProp: 0.934031044843\n",
      "Loss_RMSProp: 0.905223623093\n",
      "Loss_RMSProp: 0.878648601662\n",
      "Loss_RMSProp: 0.854035703634\n",
      "Loss_RMSProp: 0.83065046882\n",
      "Loss_RMSProp: 0.808762516889\n",
      "Loss_RMSProp: 0.788704828057\n",
      "Loss_RMSProp: 0.769816330562\n",
      "Loss_RMSProp: 0.75254644896\n",
      "Loss_RMSProp: 0.736197361685\n",
      "Loss_RMSProp: 0.721501870287\n",
      "Loss_RMSProp: 0.707600863254\n",
      "Loss_RMSProp: 0.695184552079\n",
      "Loss_RMSProp: 0.68353930082\n",
      "Loss_RMSProp: 0.673158764159\n",
      "Loss_RMSProp: 0.664150579669\n",
      "Loss_RMSProp: 0.654668824778\n",
      "Loss_RMSProp: 0.646942521231\n",
      "Loss_RMSProp: 0.639527339954\n",
      "Loss_RMSProp: 0.632986140543\n",
      "Loss_RMSProp: 0.626994137104\n",
      "Loss_RMSProp: 0.62163988839\n",
      "Loss_RMSProp: 0.616874329179\n",
      "Loss_RMSProp: 0.612632812471\n",
      "Loss_RMSProp: 0.609173496289\n",
      "Loss_RMSProp: 0.606521055749\n",
      "Loss_RMSProp: 0.602745228332\n",
      "Loss_RMSProp: 0.59991809683\n",
      "Loss_RMSProp: 0.597833706399\n",
      "Loss_RMSProp: 0.59631404586\n",
      "Loss_RMSProp: 0.59418669661\n",
      "Loss_RMSProp: 0.593135018457\n",
      "Loss_RMSProp: 0.591720973559\n",
      "Loss_RMSProp: 0.592525053588\n",
      "Loss_RMSProp: 0.590154506144\n",
      "Loss_RMSProp: 0.589205151768\n",
      "Loss_RMSProp: 0.588434235296\n",
      "Loss_RMSProp: 0.588189541248\n",
      "Loss_RMSProp: 0.587533141456\n",
      "Loss_RMSProp: 0.587284769463\n",
      "Loss_RMSProp: 0.587056156346\n",
      "Loss_RMSProp: 0.586651763327\n",
      "Loss_RMSProp: 0.586344358816\n",
      "Loss_RMSProp: 0.586479926838\n",
      "Loss_RMSProp: 0.586408572361\n",
      "Loss_RMSProp: 0.586230968114\n",
      "Loss_RMSProp: 0.586406990525\n",
      "Loss_RMSProp: 0.586828778102\n",
      "Loss_RMSProp: 0.586247680239\n",
      "Loss_RMSProp: 0.587474685421\n",
      "Loss_RMSProp: 0.586494790575\n",
      "Loss_RMSProp: 0.586591157959\n",
      "Loss_RMSProp: 0.586476631591\n",
      "Loss_RMSProp: 0.586815460215\n",
      "Loss_RMSProp: 0.586639118365\n",
      "Loss_RMSProp: 0.588451017472\n",
      "Loss_RMSProp: 0.588675684495\n",
      "Loss_RMSProp: 0.59033972483\n",
      "Loss_RMSProp: 0.587680340341\n",
      "Loss_RMSProp: 0.586760470159\n",
      "Loss_RMSProp: 0.586161702745\n",
      "Loss_RMSProp: 0.586207338516\n",
      "Loss_RMSProp: 0.58845618282\n",
      "Loss_RMSProp: 0.587093426348\n",
      "Loss_RMSProp: 0.586834119336\n",
      "Loss_RMSProp: 0.586960670537\n",
      "Loss_RMSProp: 0.58649485828\n",
      "Loss_RMSProp: 0.586571428116\n",
      "Loss_RMSProp: 0.586542000144\n",
      "Loss_RMSProp: 0.58677218785\n",
      "Loss_RMSProp: 0.58679840181\n",
      "Loss_RMSProp: 0.586545294895\n",
      "Loss_RMSProp: 0.586458615514\n",
      "Loss_RMSProp: 0.5873441658\n",
      "Loss_RMSProp: 0.58687258005\n",
      "Loss_RMSProp: 0.588646601696\n",
      "Loss_RMSProp: 0.586950917449\n",
      "Loss_RMSProp: 0.586807118944\n",
      "Loss_RMSProp: 0.587310325232\n",
      "Loss_RMSProp: 0.586549968689\n",
      "Loss_RMSProp: 0.587601288142\n",
      "Loss_RMSProp: 0.58767969303\n",
      "Loss_RMSProp: 0.58715960774\n",
      "Loss_RMSProp: 0.58726012713\n",
      "Loss_RMSProp: 0.586555148664\n",
      "Loss_RMSProp: 0.586517230193\n",
      "Loss_RMSProp: 0.589514327286\n",
      "Loss_RMSProp: 0.58729549687\n",
      "Loss_RMSProp: 0.58824965425\n",
      "Loss_RMSProp: 0.586781758705\n",
      "Loss_RMSProp: 0.586578893405\n",
      "Loss_RMSProp: 0.588898726523\n",
      "Loss_RMSProp: 0.588772753806\n",
      "Loss_RMSProp: 0.586737240745\n",
      "Loss_RMSProp: 0.586998913137\n",
      "Loss_RMSProp: 0.587588938126\n",
      "Loss_RMSProp: 0.586753192132\n",
      "Loss_RMSProp: 0.586562520055\n",
      "Loss_RMSProp: 0.5873540234\n",
      "Loss_RMSProp: 0.587481718847\n",
      "Loss_RMSProp: 0.58859012216\n",
      "Loss_RMSProp: 0.587119547344\n",
      "Loss_RMSProp: 0.586875457041\n",
      "Loss_RMSProp: 0.587048868093\n",
      "Loss_RMSProp: 0.587430009719\n",
      "Loss_RMSProp: 0.588341783304\n",
      "Loss_RMSProp: 0.587353812476\n",
      "Loss_RMSProp: 0.587283077154\n",
      "Loss_RMSProp: 0.586858322198\n",
      "Loss_RMSProp: 0.587077086907\n",
      "Loss_RMSProp: 0.586688861487\n",
      "Loss_RMSProp: 0.586694058774\n",
      "Loss_RMSProp: 0.587829583892\n",
      "Loss_RMSProp: 0.589906512744\n",
      "Loss_RMSProp: 0.587963912268\n",
      "Loss_RMSProp: 0.588076487801\n",
      "Loss_RMSProp: 0.587311233372\n",
      "Loss_RMSProp: 0.586943238043\n",
      "Loss_RMSProp: 0.58688392314\n",
      "Loss_RMSProp: 0.586965926231\n",
      "Loss_RMSProp: 0.587255437743\n",
      "Loss_RMSProp: 0.586930164949\n",
      "Loss_AdaDelta: 25.1189070908\n",
      "Loss_AdaDelta: 25.1022749311\n",
      "Loss_AdaDelta: 25.0886691569\n",
      "Loss_AdaDelta: 25.0768050901\n",
      "Loss_AdaDelta: 25.0661749985\n",
      "Loss_AdaDelta: 25.05653874\n",
      "Loss_AdaDelta: 25.0475468349\n",
      "Loss_AdaDelta: 25.0391355238\n",
      "Loss_AdaDelta: 25.0312263985\n",
      "Loss_AdaDelta: 25.0236971766\n",
      "Loss_AdaDelta: 25.0164782616\n",
      "Loss_AdaDelta: 25.0096066322\n",
      "Loss_AdaDelta: 25.0029477247\n",
      "Loss_AdaDelta: 24.9965697756\n",
      "Loss_AdaDelta: 24.9903891562\n",
      "Loss_AdaDelta: 24.9844330425\n",
      "Loss_AdaDelta: 24.978632769\n",
      "Loss_AdaDelta: 24.9729898251\n",
      "Loss_AdaDelta: 24.9674915761\n",
      "Loss_AdaDelta: 24.9621410588\n",
      "Loss_AdaDelta: 24.956934991\n",
      "Loss_AdaDelta: 24.9517955132\n",
      "Loss_AdaDelta: 24.9467869315\n",
      "Loss_AdaDelta: 24.9418752378\n",
      "Loss_AdaDelta: 24.937056837\n",
      "Loss_AdaDelta: 24.9323577905\n",
      "Loss_AdaDelta: 24.9277505305\n",
      "Loss_AdaDelta: 24.9231935454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_AdaDelta: 24.9187248413\n",
      "Loss_AdaDelta: 24.9143140904\n",
      "Loss_AdaDelta: 24.9099806866\n",
      "Loss_AdaDelta: 24.9057080012\n",
      "Loss_AdaDelta: 24.9015015166\n",
      "Loss_AdaDelta: 24.8973635118\n",
      "Loss_AdaDelta: 24.8932876064\n",
      "Loss_AdaDelta: 24.8892767855\n",
      "Loss_AdaDelta: 24.8852737283\n",
      "Loss_AdaDelta: 24.8813474184\n",
      "Loss_AdaDelta: 24.8774641413\n",
      "Loss_AdaDelta: 24.8736488802\n",
      "Loss_AdaDelta: 24.8698807738\n",
      "Loss_AdaDelta: 24.8661462368\n",
      "Loss_AdaDelta: 24.8624453258\n",
      "Loss_AdaDelta: 24.8587847188\n",
      "Loss_AdaDelta: 24.855196311\n",
      "Loss_AdaDelta: 24.851619212\n",
      "Loss_AdaDelta: 24.8480727787\n",
      "Loss_AdaDelta: 24.8445734359\n",
      "Loss_AdaDelta: 24.8410965798\n",
      "Loss_AdaDelta: 24.8376605638\n",
      "Loss_AdaDelta: 24.834260758\n",
      "Loss_AdaDelta: 24.8309084558\n",
      "Loss_AdaDelta: 24.8275831567\n",
      "Loss_AdaDelta: 24.8242828583\n",
      "Loss_AdaDelta: 24.8210014422\n",
      "Loss_AdaDelta: 24.8177588812\n",
      "Loss_AdaDelta: 24.8145407591\n",
      "Loss_AdaDelta: 24.8113502488\n",
      "Loss_AdaDelta: 24.8081857745\n",
      "Loss_AdaDelta: 24.8050572046\n",
      "Loss_AdaDelta: 24.8019388124\n",
      "Loss_AdaDelta: 24.7988435981\n",
      "Loss_AdaDelta: 24.7957820242\n",
      "Loss_AdaDelta: 24.7927319713\n",
      "Loss_AdaDelta: 24.7896928935\n",
      "Loss_AdaDelta: 24.7866936178\n",
      "Loss_AdaDelta: 24.7837312138\n",
      "Loss_AdaDelta: 24.7807762025\n",
      "Loss_AdaDelta: 24.7778448533\n",
      "Loss_AdaDelta: 24.7749389984\n",
      "Loss_AdaDelta: 24.7720376543\n",
      "Loss_AdaDelta: 24.7691582483\n",
      "Loss_AdaDelta: 24.7663083225\n",
      "Loss_AdaDelta: 24.7634767572\n",
      "Loss_AdaDelta: 24.7606593816\n",
      "Loss_AdaDelta: 24.7578665451\n",
      "Loss_AdaDelta: 24.7550836075\n",
      "Loss_AdaDelta: 24.7523093387\n",
      "Loss_AdaDelta: 24.7495491897\n",
      "Loss_AdaDelta: 24.7468380645\n",
      "Loss_AdaDelta: 24.7441309847\n",
      "Loss_AdaDelta: 24.7414272237\n",
      "Loss_AdaDelta: 24.7387414314\n",
      "Loss_AdaDelta: 24.7360701592\n",
      "Loss_AdaDelta: 24.7334134397\n",
      "Loss_AdaDelta: 24.7307771497\n",
      "Loss_AdaDelta: 24.7281496881\n",
      "Loss_AdaDelta: 24.7255502447\n",
      "Loss_AdaDelta: 24.7229723175\n",
      "Loss_AdaDelta: 24.7203918301\n",
      "Loss_AdaDelta: 24.7178246294\n",
      "Loss_AdaDelta: 24.7152820404\n",
      "Loss_AdaDelta: 24.7127435774\n",
      "Loss_AdaDelta: 24.7102159756\n",
      "Loss_AdaDelta: 24.7077086132\n",
      "Loss_AdaDelta: 24.7051932254\n",
      "Loss_AdaDelta: 24.702695144\n",
      "Loss_AdaDelta: 24.7002145957\n",
      "Loss_AdaDelta: 24.6977432952\n",
      "Loss_AdaDelta: 24.6952792841\n",
      "Loss_AdaDelta: 24.692838019\n",
      "Loss_AdaDelta: 24.6903956471\n",
      "Loss_AdaDelta: 24.6879614602\n",
      "Loss_AdaDelta: 24.6855585784\n",
      "Loss_AdaDelta: 24.683164631\n",
      "Loss_AdaDelta: 24.6807732988\n",
      "Loss_AdaDelta: 24.6783935634\n",
      "Loss_AdaDelta: 24.6760261526\n",
      "Loss_AdaDelta: 24.6736589946\n",
      "Loss_AdaDelta: 24.6713100621\n",
      "Loss_AdaDelta: 24.6689655418\n",
      "Loss_AdaDelta: 24.6666389542\n",
      "Loss_AdaDelta: 24.6643155893\n",
      "Loss_AdaDelta: 24.6620006352\n",
      "Loss_AdaDelta: 24.6597020227\n",
      "Loss_AdaDelta: 24.65741183\n",
      "Loss_AdaDelta: 24.6551326328\n",
      "Loss_AdaDelta: 24.652867743\n",
      "Loss_AdaDelta: 24.6506022491\n",
      "Loss_AdaDelta: 24.6483599538\n",
      "Loss_AdaDelta: 24.6461205171\n",
      "Loss_AdaDelta: 24.6438814645\n",
      "Loss_AdaDelta: 24.6416515519\n",
      "Loss_AdaDelta: 24.6394209138\n",
      "Loss_AdaDelta: 24.6371998652\n",
      "Loss_AdaDelta: 24.6349901754\n",
      "Loss_AdaDelta: 24.6327899918\n",
      "Loss_AdaDelta: 24.6306021014\n",
      "Loss_AdaDelta: 24.6284186418\n",
      "Loss_AdaDelta: 24.6262517519\n",
      "Loss_AdaDelta: 24.6240985622\n",
      "Loss_AdaDelta: 24.6219471616\n",
      "Loss_AdaDelta: 24.6198005637\n",
      "Loss_AdaDelta: 24.6176491246\n",
      "Loss_AdaDelta: 24.6155111185\n",
      "Loss_AdaDelta: 24.6133876941\n",
      "Loss_AdaDelta: 24.6112647763\n",
      "Loss_AdaDelta: 24.6091604526\n",
      "Loss_AdaDelta: 24.6070623391\n",
      "Loss_AdaDelta: 24.6049763183\n",
      "Loss_AdaDelta: 24.6028880749\n",
      "Loss_AdaDelta: 24.6008086678\n",
      "Loss_AdaDelta: 24.5987333899\n",
      "Loss_AdaDelta: 24.5966657795\n",
      "Loss_AdaDelta: 24.5945903713\n",
      "Loss_AdaDelta: 24.5925356743\n",
      "Loss_AdaDelta: 24.5904889853\n",
      "Loss_AdaDelta: 24.5884417272\n",
      "Loss_AdaDelta: 24.5864084925\n",
      "Loss_AdaDelta: 24.5843848586\n",
      "Loss_AdaDelta: 24.5823541416\n",
      "Loss_AdaDelta: 24.580331229\n",
      "Loss_AdaDelta: 24.5783220117\n",
      "Loss_AdaDelta: 24.5763213077\n",
      "Loss_AdaDelta: 24.5743289886\n",
      "Loss_AdaDelta: 24.5723332042\n",
      "Loss_AdaDelta: 24.5703488363\n",
      "Loss_AdaDelta: 24.5683719296\n",
      "Loss_AdaDelta: 24.5664007242\n",
      "Loss_AdaDelta: 24.5644298104\n",
      "Loss_AdaDelta: 24.5624612196\n",
      "Loss_AdaDelta: 24.5605130014\n",
      "Loss_AdaDelta: 24.5585730037\n",
      "Loss_AdaDelta: 24.5566209149\n",
      "Loss_AdaDelta: 24.5546799223\n",
      "Loss_AdaDelta: 24.5527505583\n",
      "Loss_AdaDelta: 24.5508185281\n",
      "Loss_AdaDelta: 24.5488945829\n",
      "Loss_AdaDelta: 24.5469804316\n",
      "Loss_AdaDelta: 24.5450521237\n",
      "Loss_AdaDelta: 24.5431423586\n",
      "Loss_AdaDelta: 24.5412539579\n",
      "Loss_AdaDelta: 24.5393560341\n",
      "Loss_AdaDelta: 24.5374729132\n",
      "Loss_AdaDelta: 24.5355825829\n",
      "Loss_AdaDelta: 24.5336913227\n",
      "Loss_AdaDelta: 24.5318097561\n",
      "Loss_AdaDelta: 24.52994223\n",
      "Loss_AdaDelta: 24.5280760396\n",
      "Loss_AdaDelta: 24.5262188207\n",
      "Loss_AdaDelta: 24.5243599921\n",
      "Loss_AdaDelta: 24.5224995378\n",
      "Loss_AdaDelta: 24.5206612707\n",
      "Loss_AdaDelta: 24.5188131851\n",
      "Loss_AdaDelta: 24.5169729905\n",
      "Loss_AdaDelta: 24.5151374033\n",
      "Loss_AdaDelta: 24.5132980737\n",
      "Loss_AdaDelta: 24.511476793\n",
      "Loss_AdaDelta: 24.5096625246\n",
      "Loss_AdaDelta: 24.5078454877\n",
      "Loss_AdaDelta: 24.5060348545\n",
      "Loss_AdaDelta: 24.504226447\n",
      "Loss_AdaDelta: 24.502426263\n",
      "Loss_AdaDelta: 24.5006334285\n",
      "Loss_AdaDelta: 24.4988269252\n",
      "Loss_AdaDelta: 24.4970461325\n",
      "Loss_AdaDelta: 24.4952676336\n",
      "Loss_AdaDelta: 24.4934816999\n",
      "Loss_AdaDelta: 24.4916942861\n",
      "Loss_AdaDelta: 24.489912695\n",
      "Loss_Adam: 11.0559390698\n",
      "Loss_Adam: 5.23062182534\n",
      "Loss_Adam: 3.00197978159\n",
      "Loss_Adam: 2.30249726034\n",
      "Loss_Adam: 2.10235643017\n",
      "Loss_Adam: 2.05571589619\n",
      "Loss_Adam: 2.03883865878\n",
      "Loss_Adam: 2.00767127482\n",
      "Loss_Adam: 1.9452422386\n",
      "Loss_Adam: 1.85509610394\n",
      "Loss_Adam: 1.74511274386\n",
      "Loss_Adam: 1.62451745953\n",
      "Loss_Adam: 1.5036855891\n",
      "Loss_Adam: 1.39102901122\n",
      "Loss_Adam: 1.29105436896\n",
      "Loss_Adam: 1.20610763847\n",
      "Loss_Adam: 1.13547555022\n",
      "Loss_Adam: 1.07654168837\n",
      "Loss_Adam: 1.02587597395\n",
      "Loss_Adam: 0.980613487991\n",
      "Loss_Adam: 0.938673302131\n",
      "Loss_Adam: 0.898283906971\n",
      "Loss_Adam: 0.859204090712\n",
      "Loss_Adam: 0.821586790529\n",
      "Loss_Adam: 0.787419309623\n",
      "Loss_Adam: 0.757075308505\n",
      "Loss_Adam: 0.731607450596\n",
      "Loss_Adam: 0.711264637419\n",
      "Loss_Adam: 0.695754750068\n",
      "Loss_Adam: 0.684263860336\n",
      "Loss_Adam: 0.675914457152\n",
      "Loss_Adam: 0.669735076662\n",
      "Loss_Adam: 0.664854082083\n",
      "Loss_Adam: 0.660553135574\n",
      "Loss_Adam: 0.656045590995\n",
      "Loss_Adam: 0.651193414876\n",
      "Loss_Adam: 0.645992777529\n",
      "Loss_Adam: 0.640620982061\n",
      "Loss_Adam: 0.635404832116\n",
      "Loss_Adam: 0.630292364915\n",
      "Loss_Adam: 0.625254958586\n",
      "Loss_Adam: 0.620563891454\n",
      "Loss_Adam: 0.616316266459\n",
      "Loss_Adam: 0.612504068459\n",
      "Loss_Adam: 0.609187983892\n",
      "Loss_Adam: 0.606374119131\n",
      "Loss_Adam: 0.604013437353\n",
      "Loss_Adam: 0.602028391758\n",
      "Loss_Adam: 0.60038960618\n",
      "Loss_Adam: 0.598860940091\n",
      "Loss_Adam: 0.597535645078\n",
      "Loss_Adam: 0.596420287923\n",
      "Loss_Adam: 0.595380618283\n",
      "Loss_Adam: 0.594417973679\n",
      "Loss_Adam: 0.593540697205\n",
      "Loss_Adam: 0.592762730726\n",
      "Loss_Adam: 0.591983660737\n",
      "Loss_Adam: 0.591280781078\n",
      "Loss_Adam: 0.590663408244\n",
      "Loss_Adam: 0.590126781798\n",
      "Loss_Adam: 0.589651710675\n",
      "Loss_Adam: 0.589218779037\n",
      "Loss_Adam: 0.588839389492\n",
      "Loss_Adam: 0.58851650658\n",
      "Loss_Adam: 0.588242523222\n",
      "Loss_Adam: 0.58800040062\n",
      "Loss_Adam: 0.587772237535\n",
      "Loss_Adam: 0.587567936768\n",
      "Loss_Adam: 0.587396563935\n",
      "Loss_Adam: 0.587238994414\n",
      "Loss_Adam: 0.587050503344\n",
      "Loss_Adam: 0.586879956404\n",
      "Loss_Adam: 0.586720703879\n",
      "Loss_Adam: 0.586583227232\n",
      "Loss_Adam: 0.586457689737\n",
      "Loss_Adam: 0.586327307496\n",
      "Loss_Adam: 0.586214719022\n",
      "Loss_Adam: 0.586099138093\n",
      "Loss_Adam: 0.586022183477\n",
      "Loss_Adam: 0.585925621748\n",
      "Loss_Adam: 0.585848149151\n",
      "Loss_Adam: 0.585783395074\n",
      "Loss_Adam: 0.5857340704\n",
      "Loss_Adam: 0.585689166887\n",
      "Loss_Adam: 0.585667795553\n",
      "Loss_Adam: 0.585629889744\n",
      "Loss_Adam: 0.585573728619\n",
      "Loss_Adam: 0.585527132694\n",
      "Loss_Adam: 0.585513879543\n",
      "Loss_Adam: 0.585488278169\n",
      "Loss_Adam: 0.58548530835\n",
      "Loss_Adam: 0.585468492996\n",
      "Loss_Adam: 0.585458408295\n",
      "Loss_Adam: 0.58544568084\n",
      "Loss_Adam: 0.585428538628\n",
      "Loss_Adam: 0.585398084881\n",
      "Loss_Adam: 0.58537269395\n",
      "Loss_Adam: 0.585351125529\n",
      "Loss_Adam: 0.585324773825\n",
      "Loss_Adam: 0.585292997273\n",
      "Loss_Adam: 0.585273449993\n",
      "Loss_Adam: 0.585265265015\n",
      "Loss_Adam: 0.585254081136\n",
      "Loss_Adam: 0.585248576128\n",
      "Loss_Adam: 0.58523985416\n",
      "Loss_Adam: 0.585231595419\n",
      "Loss_Adam: 0.585210728147\n",
      "Loss_Adam: 0.585201685721\n",
      "Loss_Adam: 0.58519861467\n",
      "Loss_Adam: 0.585200426168\n",
      "Loss_Adam: 0.58521000721\n",
      "Loss_Adam: 0.58521600833\n",
      "Loss_Adam: 0.585220501756\n",
      "Loss_Adam: 0.585230444387\n",
      "Loss_Adam: 0.585256819211\n",
      "Loss_Adam: 0.58531463617\n",
      "Loss_Adam: 0.585383021146\n",
      "Loss_Adam: 0.585428918439\n",
      "Loss_Adam: 0.585528821631\n",
      "Loss_Adam: 0.585631211454\n",
      "Loss_Adam: 0.58575638118\n",
      "Loss_Adam: 0.585896531361\n",
      "Loss_Adam: 0.585941455189\n",
      "Loss_Adam: 0.585916566754\n",
      "Loss_Adam: 0.585798518354\n",
      "Loss_Adam: 0.585667618243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Adam: 0.585528856708\n",
      "Loss_Adam: 0.585400326688\n",
      "Loss_Adam: 0.585334979764\n",
      "Loss_Adam: 0.585335510068\n",
      "Loss_Adam: 0.585410852533\n",
      "Loss_Adam: 0.585518930364\n",
      "Loss_Adam: 0.585628198304\n",
      "Loss_Adam: 0.585711950038\n",
      "Loss_Adam: 0.585804261204\n",
      "Loss_Adam: 0.58579690003\n",
      "Loss_Adam: 0.585750032447\n",
      "Loss_Adam: 0.585669530869\n",
      "Loss_Adam: 0.585560013323\n",
      "Loss_Adam: 0.585490260123\n",
      "Loss_Adam: 0.585437621881\n",
      "Loss_Adam: 0.585391999322\n",
      "Loss_Adam: 0.585361159757\n",
      "Loss_Adam: 0.585335656759\n",
      "Loss_Adam: 0.585313055652\n",
      "Loss_Adam: 0.58530029767\n",
      "Loss_Adam: 0.58529709648\n",
      "Loss_Adam: 0.585297868311\n",
      "Loss_Adam: 0.585295029149\n",
      "Loss_Adam: 0.585292184266\n",
      "Loss_Adam: 0.585295620756\n",
      "Loss_Adam: 0.585308449121\n",
      "Loss_Adam: 0.585320345421\n",
      "Loss_Adam: 0.585370527773\n",
      "Loss_Adam: 0.585441702738\n",
      "Loss_Adam: 0.585490870873\n",
      "Loss_Adam: 0.585521878651\n",
      "Loss_Adam: 0.585542169833\n",
      "Loss_Adam: 0.585550784367\n",
      "Loss_Adam: 0.585580431729\n",
      "Loss_Adam: 0.585616176086\n",
      "Loss_Adam: 0.585643724557\n",
      "Loss_Adam: 0.585662145348\n",
      "Loss_Adam: 0.585653701899\n",
      "Loss_Adam: 0.585631339593\n",
      "Loss_Adam: 0.58558824249\n",
      "Loss_Adam: 0.585519693109\n",
      "Loss_Adam: 0.585478007845\n",
      "Loss_Adam: 0.585438073962\n",
      "Loss_Adam: 0.585401327097\n",
      "Loss_Adam: 0.585362807076\n",
      "Loss_Adam: 0.58532858717\n",
      "Loss_Adam: 0.585311822614\n",
      "Loss_Adam: 0.585305856843\n",
      "Loss_Adam: 0.585292277991\n",
      "Loss_Adam: 0.585280236992\n",
      "Loss_Adam: 0.585270957426\n",
      "Loss_Adam: 0.585262802251\n",
      "Loss_Adam: 0.585252029152\n",
      "Loss_Adam: 0.585238911713\n",
      "Loss_Adam: 0.585226053102\n",
      "Loss_Adam: 0.585213240801\n",
      "Loss_Adam: 0.585205668378\n",
      "Loss_Adam: 0.585203388087\n",
      "Loss_Adam: 0.585204144078\n",
      "Loss_Adam: 0.585208026752\n",
      "Loss_Adam: 0.585211918788\n",
      "Loss_Adam: 0.585220598109\n",
      "Loss_Adam: 0.585233217705\n",
      "Loss_Adam: 0.585244370989\n",
      "Loss_Adam: 0.585256062189\n",
      "Loss_Adam: 0.585267559514\n",
      "Loss_Adam: 0.585279200134\n",
      "Loss_Adam: 0.585296666187\n",
      "Loss_Adam: 0.58531129503\n",
      "Loss_Adam: 0.585333391912\n",
      "Loss_Adam: 0.585358928498\n",
      "Loss_Adam: 0.58537326909\n",
      "Loss_Adam: 0.585398539163\n",
      "Loss_Adam: 0.585418979002\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd8VFX6+PHPM5NMKi0QukoRVmoC\nUkURxcJiRVcUWUDRL6sri6BfFNey6k9cBMvK17aoWAFZxMK6a1cWcUGJEjqIIAhShNDTM3N+f5yb\nkAohTMvM89Z5zZ1bn7kJ98k5595zxBiDUkqp6OUKdQBKKaVCSxOBUkpFOU0ESikV5TQRKKVUlNNE\noJRSUU4TgVJKRTlNBEopFeU0ESilVJTTRKCUUlEuJtQBVEejRo1Mq1atQh2GUkrVKt99991eY0zq\n8darFYmgVatWZGRkhDoMpZSqVURka3XW06ohpZSKcpoIlFIqymkiUEqpKFcr2giUUsFVWFjI9u3b\nycvLC3Uoqhri4+Np2bIlsbGxNdpeE4FSqoLt27dTp04dWrVqhYiEOhx1DMYYsrKy2L59O61bt67R\nPrRqSClVQV5eHg0bNtQkUAuICA0bNjyp0lvAEoGInCIiX4rIOhFZIyK3O/MfFJFfRCTTeQ0OVAxK\nqZrTJFB7nOzPKpBVQ0XAncaY70WkDvCdiHzqLHvKGPN4AI8NwAc/fMDynctJSUghJSGFBgkNSqbr\nx9cnKTaJ+Jh4/YVXSkW1gCUCY8xOYKczfVhE1gEtAnW8yny48UOey3jumOu4xEVibCJJsUkkeZJK\n3kvPS4xJJD4mnriYOOJj4u20O67KeaXnl5/ncXuIdcXicXtwu9xBOhNKKVW1oDQWi0groBvwDdAP\nGCsiI4EMbKlhfyXbjAHGAJx66qk1Ou6zlzzLU4Oe4kDeAfbl7it57c/dz/68/WQXZJNdmE12QTY5\nhTl22vmcXZhNVk4W2YXZ5Bbmku/NJ68oj/yifAp9hTU7EeW/I4LH7bHJwR1bJklUd16sOxaPq3rr\nx7hiiHHFEOs+Oh3jiimz7GSWa8lK+VNycjJHjhwJyrEmT57M7NmzcbvduFwu/v73v9O7d2+Kiop4\n4IEHmDdvHklJSQBcc8013HvvvQC43W66dOlCYWEhMTExjBo1ivHjx+Ny1a7m14AnAhFJBuYD440x\nh0TkeeD/AcZ5fwIYXX47Y8wMYAZAjx49TE2P73F7aJzUmMZJjWu6iwq8Pi/53nzyi5zkUCpJ5BXl\nVTqv+HOht5ACbwEF3gIKfXa6snmVLT9ScOT42zjzDTU+ZTXmEpffkkr55W5x4xa3nXa5j86rxnTx\nNlVNn+z2x9uXJsjwtmTJEj744AO+//574uLi2Lt3LwUFBQDcd9997Nq1i1WrVhEfH8/hw4d54okn\nSrZNSEggMzMTgF9//ZXrr7+egwcP8tBDD4Xku9RUQBOBiMRik8AsY8w7AMaY3aWWvwh8EMgYAsHt\ncpPoSiQxNjHUoVTJ6/OWJIcCbwFFviKKfEUU+gpLpot8RRR6y30+xvKT2bbIHHv9/KJ8sn3ZlS4r\n9BbiNV68Pi9e46XIV4TX57yXmh+uBPFb0im9jVvcuMRVMu12OZ/LLz/Wsiq2HZA8gF1HdiEIX9/9\nNVkrs0p/oQrf71jfvdwMABp3bczAJwaCHF2nOGGW3qbAW4A4/yHw89afufnmm9m7Zy+pqanMnDmT\nU089lbfffpuHHnoIt9tNvXr1WLRoEWvWrOHGG2+koKAAn8/H/PnzadeuXYUYd+7cSaNGjYiLiwOg\nUaNGAOTk5PDiiy+yZcsW4uPjAahTpw4PPvhgpd+1cePGzJgxg549e/Lggw/Wqj8AApYIxJ6Fl4F1\nxpgnS81v5rQfAAwBVgcqhmjmdrlJcCWQEJsQ6lCCwhiDz/iqTBSVTRevV53pk93X8bav7rrFSb14\n/z7jq3Ta6/OWnI/qrFu+BPnhRR+y/dB2AA7nHybP698Hy/bl7mPt3rXHXMdnfKzcvbLMvAljJjDw\nsoFcOvRSFry1gFF/GMXjMx/nngfu4f/e/D8aN2vMkUNH+H7n90x9YipXjrqSwVcPpqigiH3efSX7\nK518WnZrycYHNtKqbSv69O/D4CsH0/Osnvyw9geaNG/CjvwdkF82URVPGwyb9m06uqyBUOQtImNj\nBqmNU6tMcifyuX58feJi4mp0nqsrkCWCfsAIYJWIZDrz/gwME5F0bNXQFuAPAYxBRQkRsX/N4sbj\n9oQ6nFqndCL1+rxs3riZM5qegcGQ/vd0jPNf8bpgL4L2/4qfT3ZdYwwiwqn1Ti2zbM33a5j9j9nE\nxMRw8w0388zkZ2iW3Iy+ffvy1//9K5cOuZRBlw2iQWIDzjrrLJ554hkO7TnEoMsG0aptq4pxGENC\nvQTe//J9lv13GUsXL+XOMXdy5/130rFLx6OxAm/Peps3ZrzBwf0HeeODN2jWohnGGHKLcivEfjj/\nMJ48T5ntjTGVfv/jKb7RJKCMMWH/OvPMM41SKnjWrl0b6hBMUlJShXkNGzY0BQUFxhhjCgoKTKNG\njUqWLV261Nx///2mZcuWZu/evcYYY3788Ufz9NNPm9atW5vPP/+8WsedN2+eufTSS012drZJSUkx\nhw4dKrO8U6dO5qeffqo0xk2bNpmUlBTj8/mq/T2NMcbn8xmfz2e8Pq/x+rymyFtkirxFptBbaLw+\nb7X2UdnPDMgw1bjG1q6mbaVUVDvrrLN46623AJg1axZnn302AJs2baJ37948/PDDNGrUiG3btrF5\n82batGnDuHHjuPzyy1m5cmWl+9ywYQMbN24s+ZyZmclpp51GYmIiN910E2PHji15atfr9ZY0JJe3\nZ88ebrnlFsaOHXvC7QMitrrJJa6jbTdOm5BLAn+Z1r6GlFJhKScnh5YtW5Z8vuOOO5g+fTqjR49m\n2rRppKam8sorrwAwceJENm7ciDGGgQMHkpaWxpQpU3jzzTeJjY2ladOmPPDAA5Ue58iRI/zpT3/i\nwIEDxMTEcPrppzNjxgzA3lZ6//3307lzZ+rUqUNCQgKjRo2iefPmAOTm5pKenl5y++iIESO44447\nAnxm/E+MCf5thieqR48eRkcoUyp41q1bR4cOHUIdhjoBlf3MROQ7Y0yP422rVUNKKRXltGpIKRUV\nsrKyGDhwYIX5n3/+OQ0bNgxBROFDE4FSKio0bNiw5ClgVZZWDSmlVJTTRKCUUlFOE4FSSkW5iE8E\nteDuWKWUCqmITgQTJ0L79qGOQilVE8nJyUE7lohw5513lnx+/PHHK/QympaWxrBhwyps++STT3LG\nGWfQpUsX0tLSuOOOOygs9M+YJcES0YkgMRE2bYIqnghXSikA4uLieOedd9i7d2+ly9etW4fP52PR\nokVkZ2eXzH/hhRf45JNPWLp0KatWrWLZsmU0btyY3NzcYIXuFxF9+2irVrZqaNs2aNs21NEoVUuN\nHw/+vu0yPR3+9rcT3mzr1q2MHj2aPXv2lHQxceqppzJv3rwaj0cAEBMTw5gxY3jqqaeYPHlyheWz\nZ89mxIgRrFu3jgULFpSUDCZPnsyiRYuoX78+AB6Ph0mTJp3w9wq1iC4RtG5t33/6KbRxKKX8Y+zY\nsYwcOZKVK1cyfPhwxo0bB8DDDz/Mxx9/zIoVK1iwYAFg/1q//fbbyczMJCMjo0y/RZW57bbbmDVr\nFgcPHqywbO7cuVx77bUMGzaMOXPmAHD48GGOHDlC6+ILTS0W0SUCTQRK+UEN/nIPlCVLlvDOO+8A\nMGLECO666y4A+vXrxw033MDQoUO56qqrAOjbty+TJ09m+/btXHXVVVWWBorVrVuXkSNHMn36dBIS\njg7otGzZMlJTUznttNNo2bIlo0ePZv/+/bjdZYch/fjjj7n77rs5cOAAs2fP5qyzzvL31w+YiC4R\ntGwJMTGaCJSKVMUX4hdeeIFHHnmEbdu2kZ6eTlZWFtdffz0LFiwgISGBiy++mC+++OK4+xs/fjwv\nv/xymXaAOXPmsH79elq1akXbtm05dOgQ8+fPp27duiQlJfGTc4G5+OKLyczMpHPnzlV2VR2uIjoR\nuN1w6qmwZUuoI1FK+UMgxiMoLSUlhaFDh/Lyyy8D4PP5mDdvHitXrmTLli1s2bKF999/v6R66J57\n7uHWW2/lwIEDgB3oq3jsgtokoquGwDYYa4lAqdonWOMRlHfnnXfyzDPPALBo0SJatGhBixYtSpb3\n79+ftWvXsnPnTm699VZycnLo3bs3cXFxJCcn069fP7p16+bHMxF4ET8ewc03wwcfwK5dfg5KqQim\n4xHUPjoewTG0bg27d0NOTqgjUUqp8BTxVUPFdw5t3Qr6B45S0UvHI6haxCeCVq3s+08/aSJQKprp\neARVi4qqIYDNm0Mbh1JKhauITwRNm0KdOrBhQ6gjUUqp8BTxiUAEzjgD1q0LdSRKKRWeIj4RgG0b\n0ESgVO0SzG6o3W436enpdO7cmcsuu6zkAbEtW7YgItx///0l6+7du5fY2FjGjh0LwIYNGxgwYADp\n6el06NCBMWPGALBw4ULq1atHt27d6NChAw899FDQvs+JippEsGMHVNKXlFJKkZCQQGZmJqtXryYl\nJYVnn322ZFmbNm344IMPSj7PmzePTp06lXweN24cEyZMIDMzk3Xr1vGnP/2pZNk555zD8uXLycjI\n4M033+S7774rc9yioqIAfqvqi5pEALB+fWjjUEqdnK1btzJw4EC6du3KwIED+fnnnwF7ce7cuTNp\naWn0798fgDVr1tCrVy/S09Pp2rUrGzdurNYx+vbtyy+//FLyOSEhgQ4dOlD8UOvcuXMZOnRoyfKd\nO3eWeQK6S5cuFfaZlJTEmWeeyaZNm3j11Ve55ppruOyyy7joooswxjBx4kQ6d+5Mly5dmDt3LmBL\nFP3792fIkCF07NiRW265BZ/Pd4JnrHoi/vZRgI4d7fu6ddC7d2hjUaq2CaPhCEq6oR41ahQzZ85k\n3LhxvPfeeyXdULdo0aKkWqe4G+rhw4dTUFCA1+s97v69Xi+ff/45N910U5n51113HW+99RZNmzbF\n7XbTvHlzduzYAcCECRM4//zzOeuss7jooou48cYbS8YnKJaVlcXSpUu5//77WbZsGUuWLGHlypWk\npKQwf/58MjMzWbFiBXv37qVnz54lyezbb79l7dq1nHbaaQwaNIh33nmH3/3udyd+4o4jKkoErVuD\nx6PtBErVdkuWLOH6668HbDfUixcvBo52Q/3iiy+WXPD79u3Lo48+ymOPPcbWrVvLdC1dXm5uLunp\n6TRs2JB9+/Zx4YUXllk+aNAgPv30U+bMmcO1115bZtmNN97IunXruOaaa1i4cCF9+vQhPz8fgK++\n+opu3bpx0UUXMWnSpJIqpQsvvJCUlBQAFi9ezLBhw3C73TRp0oRzzz2XZcuWAdCrVy/atGmD2+1m\n2LBhJd/X36KiRBATY8cu1kSg1IkLo+EIKijdDfU333zDv/71L9LT08nMzOT666+nd+/e/Otf/+Li\niy/mpZde4vzzz690P8VtBAcPHuTSSy/l2WefLRn0BuzIY2eeeSZPPPEEa9as4Z///GeZ7Zs3b87o\n0aMZPXo0nTt3ZvXq1YBtIyjdvlAsKSmpZPpY/b2VHu+gss/+ErASgYicIiJfisg6EVkjIrc781NE\n5FMR2ei8NwhUDKXpnUNK1X6B7oa6Xr16TJ8+nccff7zCAPR33nknjz32WIXuKD766KOSdXft2kVW\nVlaZ3kqPp3///sydOxev18uePXtYtGgRvXr1AmzV0E8//YTP52Pu3Lkl39ffAlk1VATcaYzpAPQB\nbhORjsAk4HNjTDvgc+dzwHXsaAeyLzXehFIqjBV3Q138evLJJ5k+fTqvvPIKXbt25Y033uDpp58G\nbDfUXbp0oXPnzvTv35+0tDTmzp1L586dSU9PZ/369YwcObJax+3WrRtpaWklCadYp06dGDVqVIX1\nP/nkk5KG6osvvphp06bRtGnTan/PIUOG0LVrV9LS0jj//POZOnVqyfZ9+/Zl0qRJdO7cmdatWzNk\nyJBq7/dEBK0bahF5H3jGeQ0wxuwUkWbAQmPMb461bU27oTY//IB3+05izj+X996DIUNg6VJtMFbq\neLQb6tBbuHAhjz/+eKVVS5UJ+26oRaQV0A34BmhijNkJ4Lw3rmKbMSKSISIZe/bsqdFxtw4ZT8EF\nlwD2LgXw/90PSilV2wW8sVhEkoH5wHhjzKHqNnYYY2YAM8CWCGp07Pg4XMY+sHHaaVC/PixfXpM9\nKaVqu9rWDfWAAQMYMGBAUI4V0EQgIrHYJDDLGPOOM3u3iDQrVTX0a8ACiI8nhiKMzyAuIT1dSwRK\nRSvthrpqgbxrSICXgXXGmCdLLVoAFLe4jALeD1QMroQ43HgpzCkAbPXQypVQjedKlFIqagSyjaAf\nMAI4X0QynddgYApwoYhsBC50PgeEJMQjQMEBO05lejrk5sIPPwTqiEopVfsErGrIGLMYqKpBoGJF\nXQC4EuMBKDyQDS0b0K2bnZ+ZqaOVKaVUsYjuYsKVlAg4iQB78fd4oFwHgEopFdUiPBHEAVC4/wgA\nsbHQrRt8+20oo1JKVUcwxyMAePfddxER1h+jm+IbbriBt99++5j7ueGGG2jdujVpaWm0b9+ekSNH\nlunNtCoDBgwo6eH00UcfPbHgT1JEJwJ3si0RFB3KKZnXu7ctEYRJN+BKqTAxZ84czj777ApPFNfE\ntGnTWLFiBRs2bKBbt26cd955FBQUVHt7TQR+5K5jexssPHCkZF7v3pCTA06fUEqpWiRQ4xEcOXKE\nr7/+mpdffrlMIjDGMHbsWDp27Mgll1zCr78evdv94YcfpmfPnnTu3JkxY8ZU2nmciDBhwgSaNm3K\nhx9+CNguKfr27Uv37t255pprOHLkSJltJk2aVNIb6vDhwwG48sorOfPMM+nUqRMzZsyo4dmrWkT3\nPuquY3v48x7OLZlX3L3EN98cfdpYKVW18R+NJ3OXf++/T2+azt8GnXi3poEaj+C9995j0KBBtG/f\nnpSUFL7//nu6d+/Ou+++y4YNG1i1ahW7d++mY8eOjB49uiSWBx54ALBdYn/wwQdcdtllle6/e/fu\nrF+/nn79+vHII4/w2WefkZSUxGOPPcaTTz5Zsh+AKVOm8Mwzz5R55mHmzJmkpKSQm5tLz549ufrq\nq/36EFxElwhinBKBt1TVUJs20LChthMoVRsFajyCOXPmcN111wF2EJo5c+YAsGjRopKxApo3b16m\nG+svv/yS3r1706VLF7744gvWrFlT5f6LSwtLly5l7dq19OvXj/T0dF577TW2bt163O89ffp00tLS\n6NOnD9u2bav2aGvVFdElgpg6ThvB4aOJQAR69bIlAqXU8dXkL/dg8cd4BFlZWXzxxResXr0aEcHr\n9SIiTJ06tcwxSsvLy+OPf/wjGRkZnHLKKTz44IPk5eVVGefy5csZOHAgxhguvPDCkkRTHQsXLuSz\nzz5jyZIlJCYmMmDAgGMeqyYiukTgrmsTgfdITpn5ffrA2rU6mL1StU0gxiN4++23GTlyJFu3bmXL\nli1s27aN1q1bs3jxYvr3789bb72F1+tl586dfPnllwAlF+JGjRpx5MiRKu8kMsYwffp0du7cyaBB\ng+jTpw9ff/01P/74I2C72v6hkidcY2NjS8Y4OHjwIA0aNCAxMZH169ezdOnSkziDlYvoROBKsA+U\neY/klpnfrx8YA//9byiiUkpVR7DGI5gzZ06Ffv6vvvpqZs+ezZAhQ2jXrh1dunTh1ltv5dxzzwWg\nfv36/M///A9dunThyiuvpGfPnmW2nzhxYsnto8uWLePLL7/E4/GQmprKq6++yrBhw+jatSt9+vSp\n9HbVMWPG0LVrV4YPH86gQYMoKiqia9eu3H///fTp08cfp7eMoI1HcDJqOh4By5ZBr158+9v76fXv\nh0tmZ2fbnkgnToQg36WlVK2g4xHUPmE/HkHIeDwAmJz8MrOTkqB7dwjQONBKKVWrRHRjMXH2yWJv\nTsWGlXPOgWeegfz8ktWUUhGsto1HEEyRnQicEgFVJIInnrC1RwEaD1opFUZ0PIKqRXbVkPOnvq+S\nW62KL/6LFgUzIKWUCj+RnQiK2why8yssatgQunQB524wpZSKWpGdCIor//MrJgKACy6Ar76yg9Uo\npVS0iuxEUFwiOEYiyM+Hr78OZlBKKRVeoiIRVFUi6N/fjlHw2WdBjEkpVS21dTyC2iiyE4HLhc/l\nRgoLML6KD84lJ0PfvvDppyGITSkVVvw5HkFtE9mJADDuWNx4KcwtrHT5BRfA8uWwd2+QA1Oq1hgP\nDPDza3yNIqkN4xEMGDCACRMm0L9/fzp06MCyZcu46qqraNeuHffdd1+NvnegRX4iiPUQg5fC7MoT\nwYUX2n6H9O4hpcJf8XgEK1euZPjw4YwbNw6gZDyCFStWsGDBAuDoeASZmZlkZGTQsmXLKvdb2XgE\nQJnxCF588UX+W6qDsrFjx7Js2TJWr15Nbm4uH3zwQckyj8fDokWLuOWWW7jiiit49tlnWb16Na++\n+ipZWVmBODUnJbIfKANMrC0RFBwpIKlxUoXlPXpAvXq2euiaa0IQoFJhL3y6oV6yZAnvvPMOYMcj\nuOuuu4Cj4xEMHTqUq666CrDjEUyePJnt27eX/EVelTlz5jB+vC2lFI9H0L179+OORzB16lRycnLY\nt28fnTp1KhmY5vLLLwegS5cudOrUiWbNmgHQpk0btm3bFnZPMkdBIvDgchJBZWJi4LzztMFYqdoo\nXMcjiHNuXXe5XCXTxZ+LwnDA9IivGsLjIYYiCrKrHjj6ggvgp59g06YgxqWUOmG1bTyC2iLiSwTE\nxZVUDVXlggvs+2efQdu2QYpLKXVMxeMRFLvjjjuYPn06o0ePZtq0aaSmpvLKK68Atv//jRs3Yoxh\n4MCBpKWlMWXKFN58801iY2Np2rRpmXGBS5szZw6TJk0qM694PILnnnuOL774gi5dutC+fftKxyNo\n1apVhfEIapvIHo8AKOzQhc3rC/C98x4dhlTev7ox0KqV7Zr63XdPIlClIoSOR1D76HgExyDx8fb2\n0SruGgI7jvGll8Inn2h3E0qp6BMFiSDOthEco2oI4PLLIScHvvgiSIEppYIqKyuL9PT0Cq9wvJ0z\n2CK+jUASbYkg/3Dl3UwUGzAA6tSB99+HSy4JTmxKqeDR8QiqFvklggSbCAoOH7tEEBcHgwbBP/8J\nPl+QglMqjNWG9kNlnezPKvITgcdDjMtH3sGKg9OUd/nlsGsX1LBdWqmIER8fT1ZWliaDWsAYQ1ZW\nFvHx8TXeR8CqhkRkJnAp8KsxprMz70Hgf4A9zmp/Nsb8O1AxABAXR6zLR8GhY5cIAAYPBrfbVg/1\n6hXQqJQKay1btmT79u3s2bPn+CurkIuPjz9mFxrHE8g2gleBZ4DXy81/yhjzeACPW5bHg1u81SoR\npKTYsYwXLIDJk4MQm1JhKjY2ltatW4c6DBUkAasaMsYsAvYFav/VFhdHDF7yDx27sbjY5ZfD6tWw\neXOA41JKqTARijaCsSKyUkRmikiDgB/N47F3DR2sfiIAWypQSqloEOxE8DzQFkgHdgJPVLWiiIwR\nkQwRyTipesq4OFymqNolgrZtoVMncDo4VEqpiBfURGCM2W2M8RpjfMCLQJVNssaYGcaYHsaYHqmp\nqTU/qMeD21dUrTaCYtdcA4sXwy+/1PywSilVWwQ1EYhIs1IfhwCrA37QuDhcviLyTyARXHut7X9o\n3rwAxqWUUmEiYIlAROYAS4DfiMh2EbkJmCoiq0RkJXAeMCFQxy/hDGBv8vLxFnqrtckZZ0B6Osyd\nG8jAlFIqPATs9lFjzLBKZr8cqONVyRkUwu3cOZTYMLFam117LdxzD2zZYnsmVUqpSBXxTxYXlwjc\nVL/BGGwiAPjHPwIRlFJKhY8oSgTVv4UUoHVr+3SxMxiSUkpFrMhPBE7V0Ik8VFbsuutg+XL44YdA\nBKaUUuEh8hNBqRLBidxCCvY2UtBGY6VUZIv8RFDSWHxibQQALVvavodmzbK3kyqlVCSqViIQkdtF\npK5YL4vI9yJyUaCD84sathEUGzUKNmyAb7/1d2BKKRUeqlsiGG2MOQRcBKQCNwJTAhaVP5W7ffRE\nXXMNJCTAq6/6OS6llAoT1U0E4rwPBl4xxqwoNS+8OSWCWHf1Bqcpr25duPpqmDMH8k58c6WUCnvV\nTQTficgn2ETwsYjUAWrHgI5OiSA+0VWjEgHADTfAwYN2wBqllIo01U0ENwGTgJ7GmBwgFls9FP6c\nEkFCoqtGbQQA550Hp5yi1UNKqchU3UTQF9hgjDkgIr8H7gMOBi4sP3JKBHEJUuMSgctlG40/+UR7\nJFVKRZ7qJoLngRwRSQPuArZScQjK8OSUCOLjpcYlArCJwOeDN9/0V2BKKRUeqpsIiowxBrgCeNoY\n8zRQJ3Bh+ZFTIvDEUeMSAcDpp8PZZ8PLL+szBUqpyFLdRHBYRO4BRgD/EhE3tp0g/Dklgri4mlcN\nFRszBjZuhC+/9EdgSikVHqqbCK4F8rHPE+wCWgDTAhaVPzmJwOMxNbp9tLTf/Q5SUuD55/0RmFJK\nhYdqJQLn4j8LqCcilwJ5xpja0UZQXDUUa8g/mI85iXqdhAR7K+l778HOnX6KTymlQqy6XUwMBb4F\nrgGGAt+IyO8CGZjfFCcCj8FX5KMwu/CkdveHP0BREcyc6Y/glFIq9KpbNXQv9hmCUcaYkdhB5+8P\nXFh+FBMDInhibUkgd1/uSe2ufXsYOBBmzABv9Ua+VEqpsFbdROAyxvxa6nPWCWwbWiIQF4cnxj+J\nAODWW+Hnn+HDD096V0opFXLVvZh/JCIfi8gNInID8C/g34ELy8/i4oh12x4x/JEILr8cmjbVRmOl\nVGSobmPxRGAG0BVIA2YYY+4OZGB+FRdHjNh6HH8kgthYeyvpv/+to5cppWq/alfvGGPmG2PuMMZM\nMMa8G8ig/M7PiQBs9ZDHA9On+2V3SikVMsdMBCJyWEQOVfI6LCKHghXkSYuPJ4YiwH+JoGlTGDYM\nXnkF9u/3yy6VUiokjpkIjDFxDn1jAAAZgElEQVR1jDF1K3nVMcbUDVaQJy0uDldRITHxMX5LBAAT\nJkBODrz0kt92qZRSQVc77vw5WXFxkJ9PQkqCXxNBWprtovr//s8+W6CUUrVRhCeCbUBmwBIBwPjx\nsG0bzJ/v190qpVTQRHgi+CtwYUATwaWX2p5Jn3pKeyVVStVOEZ4I4oG8gCYCl8u2FXzzDSxa5Ndd\nK6VUUER4IkigdCKIT4n3eyIAuPFGaNIEHn3U77tWSqmAi/BEEA8UQUJswEoEYHslveMOO5RlRobf\nd6+UUgEVBYkASI6BvDwSGiRQlFtEYe7J9UBamVtugfr14a9/9fuulVIqoAKWCERkpoj8KiKrS81L\nEZFPRWSj894gUMe3EuxbckxJiQAgb//JDVBTmbp14U9/gnfegbVr/b57pZQKmECWCF4FBpWbNwn4\n3BjTDvjc+RxAxSUCd5lEkLvf/9VDAOPGQWIiPPZYQHavlFIBEbBEYIxZBOwrN/sK4DVn+jXgykAd\n33ISQVK5RBCAdgKARo1sZ3SzZsGPPwbkEEop5XfBbiNoYozZCeC8Nw7s4ZyqoURXUBIBwF132d5J\nH3kkYIdQSim/CtvGYhEZIyIZIpKxZ8+eGu7FKREkCni9JNSzA9kHMhE0awZ//CO88YZ2Ua2Uqh2C\nnQh2i0gzAOf916pWNMbMMMb0MMb0SE1NreHhnESQIPYtyX7dQCYCsKWC+Hh4+OGAHkYppfwi2Ilg\nATDKmR4FvB/YwzlVQ04i8HgMrhgXOXtzAnrUJk3gtttg9mxYty6gh1JKqZMWyNtH5wBLgN+IyHYR\nuQmYAlwoIhuBC53PAVRcInBiKiggqXES2buzA3tYYOJEewfRQw8F/FBKKXVSYgK1Y2PMsCoWDQzU\nMSuKL/NGXh5JTYKTCFJT7e2kU6bApEmQnh7wQyqlVI2EbWOxfzhFgTinW9D8fJKbJHNk95GgHH3i\nRPu08T33BOVwSilVIxGeCIpLBEcTQbBKBAANGsCf/wwffQRffhmUQyql1AmLjkQQVy4R/JqNCdLg\nAWPHwimnwN1363gFSqnwFOGJwKka8vjsu1M15C3wknfA//0NVab4NtJly3QUM6VUeIrwRBBn3zxe\n++6UCICgVQ8BjBgBnTrZaqJC/3d8qpRSJyXCE4EAcRB7NBEkN0kGCFqDMYDbbTui27gRnn02aIdV\nSqlqifBEAJBQJhGEokQAMHgwDBoEDz4INe4xQymlAiAKEkE8xBTZyRCVCABE4MknITsb7rsvqIdW\nSqljipJE4FTM5+eT0DABcUnQSwQAHTrYu4hefBGWLw/64ZVSqlJRkAgSwO2UCPLycLldJKYmBr1E\nUOwvf4GGDeH22/V2UqVUeIiCRBAPMQV2Mj8fgOQmySEpEYB90njyZPjqK3jzzZCEoJRSZURBIkgA\n19GqISCoTxdX5uaboW9fmDAB9u4NWRhKKQVERSKIB1fFEkGoqoYAXC6YMQMOHoT//d+QhaGUUkC0\nJALJs7ftlC4RBLGbicp07mwHsHntNfjii5CFoZRS0ZAIEmwiiIs7WiJomkxRbhH5B/NDGtl998Hp\np8Mf/gC5gR00TSmlqhQFiSAeKJsI6p1aD4CD2w6GMC5ISIAXXoAff7QNyEopFQpRkghyK08EP4c2\nEQAMHAijRtkuKFavDnU0SqloFAWJIIGqSgSHth0KYVxHPf441Ktn7yYqKgp1NEqpaBMFiaBi1VBy\n02Rcsa6wKBEANGoEzzwD33wDU6eGOhqlVLSJkkSQC3EeyLNjEIhLqNuibtgkAoDrroOhQ22ndCtW\nhDoapVQ0iYJEkAAYSPKUlAjAVg+FUyIAeO452/3EiBFlQlVKqYCKgkTgDFdZN/wTQcOG8NJLsGqV\n7ZNIKaWCIYoSQWyZRFD31Loc2n4In9cXorgqd8klttF46lT49NNQR6OUigZRkAiccYuTYyqUCIzX\ncGRn6LqaqMrf/ma7rP7972HXrlBHo5SKdFGQCJwSQZK7QiKA8HiWoLykJPjHP+DwYRg+HLzeUEek\nlIpk0ZMIkmtPIgA72P0zz9h+iP7611BHo5SKZFGQCJyqofIlglPCOxEA3HijLRH85S+waFGoo1FK\nRaooSAROiSDRVSYRxNWNI75BPPt/2h+iuI5PBJ5/Htq2hWHDdNB7pVRgRE8iSHKVPFBWrGH7huz7\nYV8IYqq+OnVse0FWln3grLAw1BEppSJNFCQCp2ooQSo8pdXojEbsXR/+Q4Slp9vnCxYutKOaKaWU\nP0VBIih111BeXpkR4xv+piGHdxwm/1D4P8b7+9/b0cyefRZefDHU0SilIkn0JILkGPD5ICenZEmj\nMxoBkPVDVigCO2FTpsCgQXDbbdp4rJTyn5AkAhHZIiKrRCRTRDICe7TiB8rc9v3w4ZIlxYmgNlQP\nAbjdMGeObTy+4gpYuzbUESmlIkEoSwTnGWPSjTE9AnuYRPuWLPb90NExCFLapiBuqTWJAKB+ffjw\nQ4iPh9/+FnbsCHVESqnaLgqqhpKAOKhTYD+WKhG4PW4atGlA1obaUTVUrFUr+Pe/Yd8+GDy4TG5T\nSqkTFqpEYIBPROQ7ERlT2QoiMkZEMkQkY89J3UAvQCokZ9uP5a6ateXOofK6dYO334Y1a2w1UW5u\nqCNSStVWoUoE/Ywx3YHfAreJSP/yKxhjZhhjehhjeqSmpp7k4RpDgpMISpUIwN45lLUxK+x6Ia2O\niy+G11+H//wHfvc7KCgIdURKqdooJInAGLPDef8VeBfoFdgjNoZ4pyuJciWC1I6pePO97NsY3g+W\nVWXYMHjhBVtV9Pvf65jHSqkTF/REICJJIlKneBq4CFgd2KM2hlinK4lyJYIWPVsA8MuyXwIbQgCN\nGQNPPAHz5tn+iTQZKKVOREwIjtkEeFdEio8/2xjzUWAP2RhcToNwuUTQqEMjYpNi2bFsB2kj0gIb\nRgDdcYd9Xu7ee20ieP11iI0NdVRKqdog6InAGLMZCPIVNxUkF+pIhaohl9tF8zOb88u3tbdEUOzP\nf7YX/7vusn0SzZ4NHk+oo1JKhbsouH0UoLF9a51coUQA0Lxnc3Zl7sJbUPtHgJk40Y5wNn8+DBkC\n2dmhjkgpFe6iKxGcllDpTfcterXAm+9l96rdQY4rMG6/Hf7+d/joIzj/fNhb++6OVUoFUXQlglPi\nqiwRABFRPVRszBhbKli5Evr1g59+CnVESqlwFV2JoHlMpSWC+q3qk9goke1Ltgc5rsC68kr49FP4\n9Vfo0we+/jrUESmlwlGUJALngbRm7kpLBCJC6/Nbs/mzzZhS3VRHgrPPhiVLoG5dOO88eOWVUEek\nlAo3UZIIEoBkmw+q6Jin7cVtObLzCL+u+jWokQXDGWfAN9/AuefC6NEwfrw+hayUOipKEgFAY2jk\nq7REAND2orYA/Pjxj8EMKmhSUmyvpbffDk8/Deeco+0GSikruhJBg8IqSwR1W9YltVMqmz7eFOS4\ngicm5uitpRs22I7r3n031FEppUItuhJBvXxbIqiiHaDtxW35+aufKciO7HqTq66C77+H9u3t9O23\nVxjOWSkVRaIoEaRCcm6F4SpL3HYbA98czbiCqWx5ZWHQowu2Nm1g8WLbXjB9OnTvDt9+G+qolFKh\nEEWJoDkkZdtxasq3E2zeDM89h/uMdiSSi2valJBEGGweDzz1lG07OHQI+va13VPo2AZKRZcoSgT9\nweWD/lRsJ5g1CwB543V2dL+E1j//h9zvomdA4EGDYPVquOkmmDbNth3897+hjkopFSxRlAjOBq/H\ndnpdukRgDLz5pr238tRTSXjsIXy4OHz7fSGLNBTq1YMZM+Djj22JoF8/uOEG2Lkz1JEppQItihJB\nPBzsChdStkSQkQE//AAjRgCQekEamxr0pP6SDzFR2GPbRRfZ0sHdd9veS9u3h6lTtTFZqUgWRYkA\nyOkHnYDCUjfQ/+tf4HLZ22cc7jGj8fjy2P3//h78GMNAnTowZYodD3nAAJsUOne2t536at+Inkqp\n44iuRFB0vn2v99XReV9/DV26QIMGJbPaPDiKg+4GeF98NbjxhZl27eCf/7SNybGxdlzkM8+08yKs\nJw6lolp0JYL4npABdJ8FLAavF5YutRXipbjjYzk04HKa7VvNL+8vC0mo4WTQINuL6Wuv2Vq1yy+3\nndh9/LEmBKUiQXQlgjp14RLgcD1gAOSlwzNH4NZtwOvA5pJVm0z7X1wYto2divHp1S4mBkaOhPXr\n4cUXYdcumyC6dbNt7YWFoY5QKVVT0ZUIEhPhcAI8eQVwNxzMhfOATh8Bo4C2QE9gNp5uZ5B9elda\nb19E5ivLQxl1WImNhZtvtu3rL71kO68bMQJat7aNygcOhDpCpdSJiq5EIAK9esHHK4DJMLE39G0O\nZANrgceBHGA4kEbitPNpwq9kjn+V/T/tD2Hg4Scuzj53sHq1bW//zW9so3Lz5jBqlH1qWauNlKod\noisRgO2gf/lyOHIEvvoKzjoLJBboANwJrALmAznIlX/DvOkivcFS3r727YgY09jfXC4YPBg+/9ye\n1lGjbEd255wDnTrBk0/Cjh2hjlIpdSzRmQi8Xtu3wrZtcMkl5VZwAVcBa4D7kKGG9FUZtOjxLu/d\nMF/bC44hPR2ef95e+F9+2T6kdued0LKlfV7v2Wdt24JSKrxIbRiRq0ePHiYjI8M/Ozt40N4q6vHY\nFtBduyA5uer1P38OzG1wAfzybXM2fXYnZ989Hpc7+nJoTaxfD//4h32tWWNLEGefbfPv4MG21CAS\n6iiVikwi8p0xpsdx14u6RACQlmbvh7zxRpg589jrFhRAi+aY+0+n4MYfiKuznx0Z6aR2fJrYxP7+\niykKrFljE8KCBZCZaee1bGkTwvnn2+qk5s1DG6NSkaS6iSA6/6w9+2z7fsMNx1/X44Fh1yN3ZRLn\nzWTbkluo3+oHYhPPJe9gL+B5YE8Ag40cnTrBQw/ZtoTt2+1dRz172q4srrsOWrSA008/mp83btQG\nZ6WCITpLBOvW2T9NH3igevUSy5fbDvsfeQTuvZefv17Lli/vocPV/yW1w16McSNyDjAA271pbyDR\nf/FGuMJCW0JYtMi23y9eDFlZdlmDBrbtIT3dPrOQnm7HYI6NDW3MStUGWjXkb1deCQsX2rELUlLI\nP5zP148tZtPH73HGVZl0GrqNBm1+RsQAsUAvbEI403m1I1oLYCfK57NtC199ZUdSy8y0NXl5eXZ5\nXJy9XbVdu6Ov9u3te+PG2uagVDFNBP62ejV07WqH9HryyZLZR7bvZ9utfyXh0wU0jf8J+gm+ixKQ\nC+OJa78PV0zxsJfJQDdsUugCtHdeqYBeuY6nqMg+xJaZaQto69fbz5s322XFEhPhlFNs20PLlken\nmzeH1FRo1Mi+6tXThKEinyaCQBgzxvav8Je/2P4WPv8cHn0UtmzB/OY3ZHfowb7NB4j/YSWN87Zh\nYuCXDs05cFYT4gZ4SOm+j/qttuL2HB0T2fg8+Aob4CtsAN5GiCsV8dTFFZsEEo9IPBAPxJV6jwU8\nzntV09VdXruvhkVFsHWrbU/YuBG2bLF3BW/fbt937Ki8x9SYGGjYsGxiqFOn6ld8/NFXXFzln2Ni\nwO0++nK5NNmo0NJEEAhFRfZx2tdfPzqvVy/b1jB4cJl/9QXfrSJ7+kvEf/guCXu2AVAoseS4E5FT\nvXjaFRLbvhB3C58tFKQCjYFGQDyY4ut/PIg7cF/JFAGFgA8wAgYwTiPtMV9S6fwK21HJusFi7OG8\nxFBk3Phw48WFFzde3PjKTftwYZCS9xM6lKl6fTl6Ipy9Ht27VOOElF7nWMfxt1pwaYgKOz4dR/9x\nt9do2+omgpga7T1axcTAK6/A0KGwZw+cdprtsL+SP/s8Z3bB89rTYP5mG6eXLCF2zRrqZWXh8xkK\nxUPugVgKD3oocnkoIhZvoQ9fdjYmJw/JzUUK8hFvIfjycbkLEHcBLncBuAzE+JyXgVgfEmMgxiCx\nBonxQaxBYn1H58XabSTW2GUxznJn2riw7RtHr1DOy7kaOMukeH6peWXWPd46wSIVr2Iu52XbmX3O\nq6jCegAGF0XGTZGJochJID7jcpKHCx9ufMaZFnGWCcZpBzJIqZe9gFeYh8u5xEup45b5EsdMEwap\nViKpCS3JhI/Eug2Ov9JJCkkiEJFBwNOAG3jJGFN7Rot3uSp5GvkYRKBjR/sq3gW2gifO78EppdSJ\nC/ptLCLiBp4Ffgt0BIaJSMdjb6WUUipQQnE/Yy/gR2PMZmNMAfAWcEUI4lBKKUVoEkELYFupz9ud\neUoppUIgFImgsmaoCi1eIjJGRDJEJGPPHu3CQSmlAiUUiWA7cEqpzy2BCj3WG2NmGGN6GGN6pKam\nBi04pZSKNqFIBMuAdiLSWkQ8wHXAghDEoZRSihDcPmqMKRKRscDH2NtHZxpj1gQ7DqWUUlZIniMw\nxvwb+Hcojq2UUqqsWtHFhIjsAbbWcPNGwF4/huMv4RoXhG9sGteJCde4IHxji7S4TjPGHLeRtVYk\ngpMhIhnV6Wsj2MI1Lgjf2DSuExOucUH4xhatcWkH+UopFeU0ESilVJSLhkQwI9QBVCFc44LwjU3j\nOjHhGheEb2xRGVfEtxEopZQ6tmgoESillDqGiE4EIjJIRDaIyI8iMimEcZwiIl+KyDoRWSMitzvz\nHxSRX0Qk03kNDkFsW0RklXP8DGdeioh8KiIbnffAj4xRNqbflDonmSJySETGh+p8ichMEflVRFaX\nmlfpORJruvM7t1JEugc5rmkist459rsiUt+Z30pEckuduxeCHFeVPzsRucc5XxtE5OIgxzW3VExb\nRCTTmR/M81XV9SF4v2PGmIh8YZ9a3gS0wQ7QuwLoGKJYmgHdnek6wA/YsRgeBP43xOdpC9Co3Lyp\nwCRnehLwWIh/jruA00J1voD+QHdg9fHOETAY+BDbuWIf4Jsgx3UREONMP1Yqrlal1wvB+ar0Z+f8\nO1iBHaeptfNv1h2suMotfwJ4IATnq6rrQ9B+xyK5RBA24x4YY3YaY753pg8D6wjvrrevAF5zpl8D\nrgxhLAOBTcaYmj5QeNKMMYuAfeVmV3WOrgBeN9ZSoL6INAtWXMaYT4wxxeNvLsV26hhUVZyvqlwB\nvGWMyTfG/AT8iP23G9S4RESAocCcQBz7WI5xfQja71gkJ4KwHPdARFoB3YBvnFljneLdzGBXwTgM\n8ImIfCciY5x5TYwxO8H+kgKNQxBXseso+48z1OerWFXnKJx+70Zj/3Is1lpElovIf0TknBDEU9nP\nLlzO1znAbmPMxlLzgn6+yl0fgvY7FsmJoFrjHgSTiCQD84HxxphDwPNAWyAd2IktmgZbP2NMd+zQ\nobeJSP8QxFApsb3TXg7Mc2aFw/k6nrD4vRORe4EiYJYzaydwqjGmG3AHMFtE6gYxpKp+dmFxvoBh\nlP2DI+jnq5LrQ5WrVjLvpM5ZJCeCao17ECwiEov9Ic8yxrwDYIzZbYzxGmN8wIsEqEh8LMaYHc77\nr8C7Tgy7i4uazvuvwY7L8Vvge2PMbifGkJ+vUqo6RyH/vRORUcClwHDjVCo7VS9ZzvR32Lr49sGK\n6Rg/u3A4XzHAVcDc4nnBPl+VXR8I4u9YJCeCsBn3wKl/fBlYZ4x5stT80vV6Q4DV5bcNcFxJIlKn\neBrb0Lgae55GOauNAt4PZlyllPkrLdTnq5yqztECYKRzZ0cf4GBx8T4YRGQQcDdwuTEmp9T8VBFx\nO9NtgHbA5iDGVdXPbgFwnYjEiUhrJ65vgxWX4wJgvTFme/GMYJ6vqq4PBPN3LBit4qF6YVvXf8Bm\n83tDGMfZ2KLbSiDTeQ0G3gBWOfMXAM2CHFcb7B0bK4A1xecIaAh8Dmx03lNCcM4SgSygXql5ITlf\n2GS0EyjE/jV2U1XnCFtsf9b5nVsF9AhyXD9i64+Lf89ecNa92vkZrwC+By4LclxV/uyAe53ztQH4\nbTDjcua/CtxSbt1gnq+qrg9B+x3TJ4uVUirKRXLVkFJKqWrQRKCUUlFOE4FSSkU5TQRKKRXlNBEo\npVSU00SgooqI/Nd5byUi1/t533+u7FhKhTu9fVRFJREZgO0N89IT2MZtjPEeY/kRY0yyP+JTKpi0\nRKCiiogccSanAOc4fc1PEBG32L78lzkdo/3BWX+A01f8bOzDO4jIe04nfWuKO+oTkSlAgrO/WaWP\n5TwBOk1EVosd++HaUvteKCJvix1DYJbzlKlSQRUT6gCUCpFJlCoROBf0g8aYniISB3wtIp846/YC\nOhvbTTLAaGPMPhFJAJaJyHxjzCQRGWuMSa/kWFdhO1tLAxo52yxylnUDOmH7ivka6Acs9v/XVapq\nWiJQyroI239LJrYL4IbY/mUAvi2VBADGicgKbH//p5RarypnA3OM7XRtN/AfoGepfW83tjO2TOyA\nKEoFlZYIlLIE+JMx5uMyM21bQna5zxcAfY0xOSKyEIivxr6rkl9q2ov+m1QhoCUCFa0OY4cFLPYx\ncKvTHTAi0t7pkbW8esB+JwmcgR0qsFhh8fblLAKuddohUrFDJga7h02lqqR/fahotRIocqp4XgWe\nxlbLfO802O6h8iE6PwJuEZGV2N4yl5ZaNgNYKSLfG2OGl5r/LtAX25OlAe4yxuxyEolSIae3jyql\nVJTTqiGllIpymgiUUirKaSJQSqkop4lAKaWinCYCpZSKcpoIlFIqymkiUEqpKKeJQCmlotz/BzH0\n7cXhZmciAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x259e7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn import datasets\n",
    "\n",
    "def data_loader(file):\n",
    "    X, Y = datasets.load_svmlight_file(file,n_features=123)\n",
    "    one = np.ones((X.shape[0], 1))\n",
    "    X = hstack([X, csr_matrix(one)]).toarray()\n",
    "    Y = np.array(Y).reshape(X.shape[0], 1)\n",
    "    return X, Y\n",
    "\n",
    "def select_sample(X,Y,n,index_list):#批处理抽出样本\n",
    "    sample_index= random.sample(index_list,n)\n",
    "    X_samples =np.ones((0,X.shape[1]))\n",
    "    Y_samples  = np.ones((0, Y.shape[1]))\n",
    "\n",
    "    for item in  sample_index:\n",
    "        X_samples = np.r_[X_samples,X[item].reshape(1,X.shape[1])]\n",
    "        Y_samples = np.r_[Y_samples, Y[item].reshape(1, Y.shape[1])]\n",
    "    return X_samples,Y_samples\n",
    "\n",
    "def SGD(theta,learning_rate,epoch=200):#得到随机梯度\n",
    "\n",
    "    for j in range(1,epoch+1):#迭代200次，每次samplenumber个样本\n",
    "     m_gradient = np.zeros(theta.shape)\n",
    "     X, Y = select_sample(X_train, Y_train, samplesnumber, index_list)  # 选取样本\n",
    "     for i in range(0,Y.shape[0]):\n",
    "\n",
    "        Xi=X[i].reshape(1,X.shape[1]).T\n",
    "        Yi=Y[i][0]\n",
    "        m_gradient += -  (Xi * Yi) / (1 + math.exp(Yi*np.dot(theta.T,Xi)[0][0]))\n",
    "\n",
    "     gradient =m_gradient/samplesnumber+  lunda * theta\n",
    "     theta =  theta -learning_rate*gradient\n",
    "     loss = loss_funtion(X_test, Y_test, theta)\n",
    "     print(\"Loss_SGD:\", loss)\n",
    "     Loss_SGD.append(loss)\n",
    "    return  Loss_SGD\n",
    "\n",
    "def NAG(theta,learning_rate,lunda,epoch=200):#NAG\n",
    "    Vt = np.zeros(theta.shape)\n",
    "    for j in range(1,epoch+1):\n",
    "      m_gradient = np.zeros(theta.shape)\n",
    "      X, Y = select_sample(X_train, Y_train, samplesnumber, index_list)  # 选取样本\n",
    "      for i in range(0,Y.shape[0]):\n",
    "       #对theta(t-1)-lengta*V(t-1)求导数\n",
    "\n",
    "         Xi = X[i].reshape(1, X.shape[1]).T\n",
    "         Yi = Y[i][0]\n",
    "         m_gradient+=-((Xi*Yi)/(1+math.exp(Yi*np.dot((theta-lengta*Vt).T,Xi)[0][0])) )\n",
    "      gradient=m_gradient/samplesnumber+lunda*(theta-lengta*Vt)\n",
    "      Vt=lengta*Vt+learning_rate*gradient\n",
    "      theta=theta-Vt\n",
    "      loss = loss_funtion(X_test, Y_test, theta)\n",
    "      print(\"Loss_NAG:\", loss)\n",
    "      Loss_NAG.append(loss)\n",
    "\n",
    "    return Loss_NAG\n",
    "\n",
    "def RMSProp(theta,epoch=200):#RMSProp\n",
    "    epsilon = 1e-8\n",
    "    lengta=0.9\n",
    "    learning_rate=0.01\n",
    "    Gt = np.zeros(theta.shape)\n",
    "    for j in range(1,epoch+1):\n",
    "       #这里的学习率 0.001\n",
    "       X, Y = select_sample(X_train, Y_train, samplesnumber, index_list)  # 选取样本\n",
    "       m_gradient = np.zeros(theta.shape)\n",
    "       for i in range(0, Y.shape[0]):\n",
    "           Xi = X[i].reshape(1, X.shape[1]).T\n",
    "           Yi = Y[i][0]\n",
    "           m_gradient += -  (Xi * Yi) / (1 + math.exp(Yi * np.dot(theta.T, Xi)[0][0]))\n",
    "\n",
    "       gradient = m_gradient / samplesnumber + lunda * theta\n",
    "       Gt=lengta*Gt+(1-lengta)*gradient*gradient\n",
    "       theta=theta-learning_rate*gradient/np.sqrt(Gt+epsilon)\n",
    "       loss = loss_funtion(X_test, Y_test, theta)\n",
    "       print(\"Loss_RMSProp:\", loss)\n",
    "       Loss_RMSProp.append(loss)\n",
    "\n",
    "    return Loss_RMSProp\n",
    "\n",
    "def AdaDelta(theta,epoch=200):#AdaDelta,无初始学习率,用sqrt{Delta_{t-1} + epsilon}来估计学习速率,lengda=0.95\n",
    "    lengda=0.999#yuan0.95\n",
    "    epsilon = 1e-8\n",
    "    delta_t = np.zeros(theta.shape)\n",
    "    Gt = np.zeros(theta.shape)\n",
    "    for j in range(1,epoch+1):\n",
    "       m_gradient = np.zeros(theta.shape)\n",
    "       X, Y = select_sample(X_train, Y_train, samplesnumber, index_list)  # 选取样本\n",
    "       for i in range(0, Y.shape[0]):\n",
    "           Xi = X[i].reshape(1, X.shape[1]).T\n",
    "           Yi = Y[i][0]\n",
    "           m_gradient += -  (Xi * Yi) / (1 + math.exp(Yi * np.dot(theta.T, Xi)[0][0]))\n",
    "\n",
    "       gradient = m_gradient / samplesnumber + lunda * theta\n",
    "       Gt =lengda* Gt + np.sum((1 - lengda) * (gradient**2))\n",
    "       delta_theta=-np.sqrt(delta_t+epsilon)/np.sqrt(Gt+epsilon)*gradient\n",
    "       theta = theta +delta_theta # 偷*learning_rate\n",
    "       delta_t=lengda*delta_t+(1-lengda)*delta_theta*delta_theta\n",
    "       loss = loss_funtion(X_test, Y_test, theta)\n",
    "       print(\"Loss_AdaDelta:\", loss)\n",
    "       Loss_AdaDelta.append(loss)\n",
    "    return Loss_AdaDelta\n",
    "\n",
    "def  Adam(theta,epoch=200):#和Adam  \\beta_1取个0.9（可能需要衰减）\n",
    "    beta_1=0.9\n",
    "    lengda=0.999\n",
    "    learning_rate=0.01\n",
    "    epsilon = 1e-8\n",
    "    m_t = np.zeros(theta.shape)\n",
    "    Gt = np.zeros(theta.shape)\n",
    "    for j in range(1,epoch+1):\n",
    "        m_gradient = np.zeros(theta.shape)\n",
    "        X, Y = select_sample(X_train, Y_train, samplesnumber, index_list)  # 选取样本\n",
    "        for i in range(0, Y.shape[0]):\n",
    "            Xi = X[i].reshape(1, X.shape[1]).T\n",
    "            Yi = Y[i][0]\n",
    "            m_gradient += -  (Xi * Yi) / (1 + math.exp(Yi * np.dot(theta.T, Xi)[0][0]))\n",
    "\n",
    "        gradient = m_gradient / samplesnumber + lunda * theta\n",
    "        m_t=beta_1*m_t+(1-beta_1)*gradient\n",
    "        Gt=lengda*Gt+(1-lengda)*gradient*gradient\n",
    "        alpha=learning_rate*math.sqrt(1-pow(lengda,i))/(1-pow(beta_1,j))\n",
    "        theta=theta-alpha*m_t/np.sqrt(Gt+epsilon)\n",
    "        loss = loss_funtion(X_test, Y_test, theta)\n",
    "        print(\"Loss_Adam:\", loss)\n",
    "        Loss_Adam.append(loss)\n",
    "\n",
    "    return Loss_Adam\n",
    "\n",
    "def loss_funtion(X,Y,theta):\n",
    "    m_loss=0\n",
    "    for i in range (0,Y.shape[0]):\n",
    "        Xi = X[i].reshape(1, X.shape[1]).T\n",
    "        Yi = Y[i][0]\n",
    "        m_loss+=math.log(1+math.exp(-Yi*np.dot(theta.T,Xi)[0][0]))\n",
    "    loss=1/Y.shape[0]*m_loss+0.5 * np.dot(theta.T, theta).sum()\n",
    "    #print(\"first: \",1/samplesnumber*m_loss)\n",
    "    #print(\"second:\",0.5 * np.dot(theta.transpose(), theta).sum())\n",
    "    return  loss\n",
    "\n",
    "def Accuracy(X,Y,theta,threshold=0.5):\n",
    "    y_prediction=X.dot(theta.transpose())\n",
    "    y_prediction[y_prediction < threshold]=-1\n",
    "    y_prediction[y_prediction>=threshold]= 1\n",
    "\n",
    "    classification = Y* y_prediction  # 可以相乘吗\n",
    "    classification[classification == -1] = 0\n",
    "    accuracy = classification.sum() / classification.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def plot_loss(Loss_SGD,Loss_NAG,Loss_RMSProp,Loss_AdaDelta,Loss_Adam):\n",
    "    plt.figure(1)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.plot(Loss_SGD, 'purple', label='Loss_SGD')\n",
    "    plt.plot(Loss_NAG,'red',label='Loss_NAG')\n",
    "    plt.plot(Loss_RMSProp, 'blue', label='Loss_RMSProp')\n",
    "    plt.plot(Loss_AdaDelta, 'green', label='Loss_AdaDelta')\n",
    "    plt.plot(Loss_Adam, 'yellow', label='Loss_Adam')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    learning_rate=0.1\n",
    "    lunda=1\n",
    "    lengta=0.9\n",
    "    epoch=50\n",
    "    X_train,Y_train=data_loader(\"Desktop./a9a.txt\")\n",
    "    X_test,Y_test=data_loader(\"Desktop./a9atesting.txt\")\n",
    "    theta = np.random.random((X_train.shape[1], 1))\n",
    "    index_list=range(0,X_train.shape[0])\n",
    "    #main函数\n",
    "    #在验证集上测试并得到不同优化方法的Loss函数值\n",
    "    samplesnumber=1000#样本数\n",
    "    Loss_SGD = []\n",
    "    Loss_NAG = []\n",
    "    Loss_RMSProp = []\n",
    "    Loss_AdaDelta=[]\n",
    "    Loss_Adam=[]\n",
    "\n",
    "   # X, Y = select_sample(X_train, Y_train, samplesnumber, index_list)  # 选取样本\n",
    "    Loss_SGD = SGD( theta, learning_rate)\n",
    "    Loss_NAG=NAG(theta,learning_rate,lunda)\n",
    "    Loss_RMSProp=RMSProp( theta)\n",
    "    Loss_AdaDelta=AdaDelta( theta)\n",
    "    Loss_Adam=Adam( theta)\n",
    "    plot_loss(Loss_SGD,Loss_NAG, Loss_RMSProp, Loss_AdaDelta, Loss_Adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
